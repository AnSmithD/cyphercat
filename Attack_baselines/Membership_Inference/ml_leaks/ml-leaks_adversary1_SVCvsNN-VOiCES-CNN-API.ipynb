{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.5 (default, Jul  6 2018, 19:12:46) \n",
      "[GCC 5.4.0 20160609]\n",
      "Pytorch: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.insert(0, '../../../Utils/')\n",
    "sys.path.insert(0, '../../../')\n",
    "import cyphercat as cc\n",
    "\n",
    "import models\n",
    "from train import *\n",
    "from metrics import * \n",
    "from SVC_Utils import *\n",
    "\n",
    "#audio\n",
    "import librosa as libr\n",
    "\n",
    "print(\"Python: %s\" % sys.version)\n",
    "print(\"Pytorch: %s\" % torch.__version__)\n",
    "\n",
    "# determine device to run network on (runs on gpu if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.001\n",
    "k = 3\n",
    "\n",
    "pretrained = False #run this with networks that have already been trained\n",
    "\n",
    "transform_type = 'SFTF' #either STFT or MFCC  \n",
    "\n",
    "data = 'VOiCES' #'Libri' or 'VOiCES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speech preprocessing\n",
    "\n",
    "class tensorToMFCC:\n",
    "    def __call__(self, y):\n",
    "#         y = y.numpy()\n",
    "        dims = y.shape\n",
    "        y = libr.feature.melspectrogram(np.reshape(y, (dims[1],)), 16000, n_mels=number_of_mels,\n",
    "                               fmax=8000)\n",
    "        y = libr.feature.mfcc(S = libr.power_to_db(y))\n",
    "        y = torch.from_numpy(y)                           \n",
    "        return y.float()\n",
    "\n",
    "class STFT:\n",
    "    def __call__(self,y):\n",
    "        dims = y.shape\n",
    "        y = np.abs(libr.core.stft(np.reshape(y, (dims[1],))))\n",
    "        y = torch.from_numpy(y).permute(1,0)\n",
    "        return y.float()\n",
    "\n",
    "if transform_type == 'SFTF':\n",
    "    target_net_type = cc.ft_cnn_classifer\n",
    "    shadow_net_type = cc.ft_cnn_classifer\n",
    "    in_size = 94# 20 forMFCC,  94 for STFT\n",
    "    transform  = STFT() ## STFT or MFCC\n",
    "elif transform_type == 'MFCC':\n",
    "    transform  = tensorToMFCC()\n",
    "    target_net_type = cc.MFCC_cnn_classifier\n",
    "    shadow_net_type = cc.MFCC_cnn_classifier\n",
    "    in_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "n_seconds = 3\n",
    "n_epochs = 15\n",
    "sampling_rate = 16000\n",
    "number_of_mels =128\n",
    "lr = 0.001\n",
    "\n",
    "# attacking means data for a target & shadow network.\n",
    "# This will also split \"out data\" from totally different speakers -- data none of the \n",
    "# other networks have seen, for training & testing the attack network. This will be\n",
    "# an equivalent amount of data to the train split as defined about\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, './../../../Utils')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load audio data: VOiCES or LibriSpeech, & Split into valid sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading splits\n",
      "Initialising VOiCESDataset with minimum length = 3s and subset = room-1\n",
      "Finished indexing data. 187260 usable files found.\n",
      "Found default splits, loading dataframe\n",
      "\t\t ---- Split 0 ---- \n",
      "\tUnique speakers \t Samples\n",
      "Male:\t\t 75\t\t 23208\n",
      "Female:\t\t 75\t\t 22800\n",
      "Total:\t\t 150\t\t 46008\n",
      "\t\t ---- Split 1 ---- \n",
      "\tUnique speakers \t Samples\n",
      "Male:\t\t 75\t\t 23208\n",
      "Female:\t\t 75\t\t 22800\n",
      "Total:\t\t 150\t\t 46008\n",
      "Finished splitting data.\n",
      "Initializing dataset\n"
     ]
    }
   ],
   "source": [
    "print('Loading splits')\n",
    "subset = 'room-1'\n",
    "if data == 'Libri':\n",
    "    dfs = cc.Libri_preload_and_split()\n",
    "    print('Initializing dataset')\n",
    "    valid_sequence_train_target = cc.LibriSpeechDataset(df=dfs[0], transform = transform)\n",
    "    valid_sequence_test_target = cc.LibriSpeechDataset(df=dfs[1], transform = transform)\n",
    "    valid_sequence_train_shadow = cc.LibriSpeechDataset(df=dfs[2], transform = transform)\n",
    "    valid_sequence_test_shadow = cc.LibriSpeechDataset(df=dfs[3], transform = transform)\n",
    "    valid_sequence_test = cc.LibriSpeechDataset(df=dfs[4], transform = transform)\n",
    "\n",
    "    print('Succesfully loaded libri-speech')\n",
    "elif data == 'VOiCES':\n",
    "    dfs = cc.Voices_preload_and_split(subset = subset)\n",
    "    print('Initializing dataset')\n",
    "    valid_sequence_train_target = cc.Voices_dataset(df=dfs[0], transform = transform)\n",
    "    valid_sequence_test_target = cc.Voices_dataset(df=dfs[1], transform = transform)\n",
    "#     valid_sequence_train_shadow = cc.Voices_dataset(df=dfs[2], transform = transform)\n",
    "#     valid_sequence_test_shadow = cc.Voices_dataset(df=dfs[3], transform = transform)\n",
    "#     valid_sequence_test = cc.Voices_dataset(df=dfs[4], transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 speakers\n",
      "187356 files\n",
      "9 tracks min\n",
      "17 tracks max\n",
      "11.466666666666667 tracks mean\n",
      "min speaker minutes 146.0639999999989\n",
      "max speaker minutes 260.18799999999635\n",
      "mean speaker minutes 162.55991877777768\n"
     ]
    }
   ],
   "source": [
    "# to look at the index file:\n",
    "\n",
    "#look at splits file for reference\n",
    "dff = pd.read_csv(os.getcwd()+'/../../../Datasets/splits/libri-train-clean-100/libri_4.csv')\n",
    "dff.head()\n",
    "\n",
    "df = pd.read_csv(os.getcwd()+'/../../../Datasets/VOiCES-room-1.index.csv')\n",
    "df.head()\n",
    "\n",
    "g = df.groupby(['id','Section']).groups\n",
    "\n",
    "dfn = pd.DataFrame(columns = ['id','Section'])\n",
    "idx = 0\n",
    "for key in g.keys():\n",
    "    dfn.at[idx,'id']=key[0]\n",
    "    dfn.at[idx,'Section']=key[1]\n",
    "    idx +=1\n",
    "\n",
    "print(len(np.unique(df.id)), 'speakers')\n",
    "print(len(df), 'files')\n",
    "print(dfn.groupby('id').count().min()[0], 'tracks min')\n",
    "print(dfn.groupby('id').count().max()[0], 'tracks max')\n",
    "print(dfn.groupby('id').count().mean()[0], 'tracks mean')\n",
    "\n",
    "print('min speaker minutes',df.groupby('id').mean()['speaker_minutes'].min())\n",
    "print('max speaker minutes',df.groupby('id').mean()['speaker_minutes'].max())\n",
    "print('mean speaker minutes',df.groupby('id').mean()['speaker_minutes'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sex</th>\n",
       "      <th>subset</th>\n",
       "      <th>filepath</th>\n",
       "      <th>length</th>\n",
       "      <th>seconds</th>\n",
       "      <th>Section</th>\n",
       "      <th>Mic</th>\n",
       "      <th>Degree</th>\n",
       "      <th>Noise</th>\n",
       "      <th>speaker_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...</td>\n",
       "      <td>247440</td>\n",
       "      <td>15.465</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>170</td>\n",
       "      <td>tele</td>\n",
       "      <td>200.33995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...</td>\n",
       "      <td>252960</td>\n",
       "      <td>15.810</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>tele</td>\n",
       "      <td>200.33995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...</td>\n",
       "      <td>247440</td>\n",
       "      <td>15.465</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>170</td>\n",
       "      <td>tele</td>\n",
       "      <td>200.33995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...</td>\n",
       "      <td>245200</td>\n",
       "      <td>15.325</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>180</td>\n",
       "      <td>tele</td>\n",
       "      <td>200.33995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...</td>\n",
       "      <td>245200</td>\n",
       "      <td>15.325</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>tele</td>\n",
       "      <td>200.33995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id sex           subset                                           filepath  \\\n",
       "0  32   F  train-clean-100  VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...   \n",
       "1  32   F  train-clean-100  VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...   \n",
       "2  32   F  train-clean-100  VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...   \n",
       "3  32   F  train-clean-100  VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...   \n",
       "4  32   F  train-clean-100  VOiCES/room-1/Volumes/Lab41-SRI-VOiCES/VOiCES_...   \n",
       "\n",
       "   length  seconds  Section  Mic  Degree Noise  speaker_minutes  \n",
       "0  247440   15.465       13    6     170  tele        200.33995  \n",
       "1  252960   15.810       11    2      40  tele        200.33995  \n",
       "2  247440   15.465       13   12     170  tele        200.33995  \n",
       "3  245200   15.325        5   12     180  tele        200.33995  \n",
       "4  245200   15.325        5    4     180  tele        200.33995  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37920 149436\n"
     ]
    }
   ],
   "source": [
    "splits = [.2,.8]\n",
    "df1 = pd.DataFrame(columns = df.columns)\n",
    "df2 = pd.DataFrame(columns = df.columns)\n",
    "# For each speaker, identify unique segments: \n",
    "for spkr_id in df.id.unique():\n",
    "    mini_df = df[df['id'] == spkr_id]\n",
    "    # Identify segments:\n",
    "    n_seg = len(mini_df.Section.unique())\n",
    "    seg1 = round(splits[0]*n_seg)\n",
    "    # Segments are not ordered in a particular way, so just pick the first few for seg1\n",
    "    seg1s = mini_df.Section.unique()[:seg1]\n",
    "    df1 = df1.append(mini_df[mini_df['Section'].isin(seg1s)])\n",
    "    df2 = df2.append(mini_df[~mini_df['Section'].isin(seg1s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Loaders for data for target model & shadow model \n",
    "target_train_loader = DataLoader(valid_sequence_train_target,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8,\n",
    "                    drop_last = True\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "\n",
    "target_test_loader = DataLoader(valid_sequence_test_target,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "# shadow_train_loader = DataLoader(valid_sequence_train_shadow,\n",
    "#                       batch_size=batch_size,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=8,\n",
    "#                     drop_last = True\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "\n",
    "\n",
    "# shadow_test_loader = DataLoader(valid_sequence_test_shadow,\n",
    "#                       batch_size=batch_size,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=8\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "\n",
    "\n",
    "# test_loader = DataLoader(valid_sequence_test,\n",
    "#                       batch_size=batch_size,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=8\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table with summary\n",
    "\n",
    "# To do: extract accuracy from train/eval funcs and automatically add to table\n",
    "summary_file = 'summary.pkl'\n",
    "columns = ['Transform','Training epochs', '# speakers','Train accuracy', 'Test accuracy', 'Attack type', 'Precision','Recall']\n",
    "\n",
    "try:\n",
    "    df = pd.read_pickle(summary_file)\n",
    "\n",
    "except:\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "df_idx = len(df)\n",
    "\n",
    "#set a bunch of known values\n",
    "df.at[df_idx,'Transform'] =transform_type\n",
    "df.at[df_idx,'Training epochs'] = n_epochs\n",
    "df.at[df_idx,'Attack type'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Targets\n",
    "The model being attacked; if network, architecture can differ from that of shadow network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150  speakers\n"
     ]
    }
   ],
   "source": [
    "#Initialize NN\n",
    "\n",
    "#in_size defined above\n",
    "n_hidden = 512\n",
    "n_classes = valid_sequence_test_target.num_speakers\n",
    "print(n_classes,' speakers')\n",
    "df.at[df_idx,'# speakers']=n_classes\n",
    "\n",
    "\n",
    "target_net = target_net_type(n_classes).to(device)\n",
    "target_net.apply(models.weights_init)\n",
    "\n",
    "target_loss = nn.CrossEntropyLoss()\n",
    "target_optim = optim.Adam(target_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-12:\n",
      "Process Process-15:\n",
      "Process Process-13:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1e8a27de234a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Train NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_test_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Train accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Test accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/fs4/home/nlopatina/cyphercat/cyphercat/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, test_loader, optimizer, criterion, n_epochs, classes, verbose)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader timed out after {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../../../cyphercat/datadefs/voices_dataset.py\", line 470, in __getitem__\n",
      "    os.path.join(DATASETS_DIR, self.datasetid_to_filepath[index]))\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 257, in read\n",
      "    subtype, endian, format, closefd) as f:\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 627, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1173, in _open\n",
      "    file_ptr = openfunction(file, mode_int, self._info)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../../../cyphercat/datadefs/voices_dataset.py\", line 470, in __getitem__\n",
      "    os.path.join(DATASETS_DIR, self.datasetid_to_filepath[index]))\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 257, in read\n",
      "    subtype, endian, format, closefd) as f:\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 627, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1173, in _open\n",
      "    file_ptr = openfunction(file, mode_int, self._info)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process Process-16:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../../../cyphercat/datadefs/voices_dataset.py\", line 470, in __getitem__\n",
      "    os.path.join(DATASETS_DIR, self.datasetid_to_filepath[index]))\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 259, in read\n",
      "    data = f.read(frames, dtype, always_2d, fill_value, out)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 865, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1310, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1319, in _cdata_io\n",
      "    frames = func(self._file, data, frames)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process Process-14:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../../../cyphercat/datadefs/voices_dataset.py\", line 470, in __getitem__\n",
      "    os.path.join(DATASETS_DIR, self.datasetid_to_filepath[index]))\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 259, in read\n",
      "    data = f.read(frames, dtype, always_2d, fill_value, out)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 865, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1310, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1319, in _cdata_io\n",
      "    frames = func(self._file, data, frames)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../../../cyphercat/datadefs/voices_dataset.py\", line 470, in __getitem__\n",
      "    os.path.join(DATASETS_DIR, self.datasetid_to_filepath[index]))\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 257, in read\n",
      "    subtype, endian, format, closefd) as f:\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 627, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1173, in _open\n",
      "    file_ptr = openfunction(file, mode_int, self._info)\n",
      "KeyboardInterrupt\n",
      "Process Process-9:\n",
      "Process Process-11:\n",
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../../../cyphercat/datadefs/voices_dataset.py\", line 470, in __getitem__\n",
      "    os.path.join(DATASETS_DIR, self.datasetid_to_filepath[index]))\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 259, in read\n",
      "    data = f.read(frames, dtype, always_2d, fill_value, out)\n",
      "  File \"../../../cyphercat/datadefs/voices_dataset.py\", line 470, in __getitem__\n",
      "    os.path.join(DATASETS_DIR, self.datasetid_to_filepath[index]))\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 865, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 259, in read\n",
      "    data = f.read(frames, dtype, always_2d, fill_value, out)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1310, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 865, in read\n",
      "    frames = self._array_io('read', out, frames)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1319, in _cdata_io\n",
      "    frames = func(self._file, data, frames)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1310, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1319, in _cdata_io\n",
      "    frames = func(self._file, data, frames)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../../../cyphercat/datadefs/voices_dataset.py\", line 470, in __getitem__\n",
      "    os.path.join(DATASETS_DIR, self.datasetid_to_filepath[index]))\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 259, in read\n",
      "    data = f.read(frames, dtype, always_2d, fill_value, out)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 865, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1310, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1319, in _cdata_io\n",
      "    frames = func(self._file, data, frames)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#file name for this set of hyperparameters\n",
    "fn = 'model_weights/CNN_voice_classifier'+data+'_target_'+transform_type+str(n_epochs-1)+'.pth'\n",
    "\n",
    "#Train NN\n",
    "if not pretrained:\n",
    "    train_accuracy, test_accuracy = cc.train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs, verbose = False) \n",
    "    df.at[df_idx,'Train accuracy'] =round(train_accuracy,4)\n",
    "    df.at[df_idx,'Test accuracy'] = round(test_accuracy,4)\n",
    "    cc.save_checkpoint(model = target_net, optimizer = target_optim,\n",
    "                           epoch = n_epochs-1, data_descriptor = data, \n",
    "                           accuracy = [train_accuracy, test_accuracy],\n",
    "                           filename = fn)\n",
    "    \n",
    "else:\n",
    "    cc.load_checkpoint(model = target_net, optimizer = target_optim, checkpoint = fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Shadow Model\n",
    "Shadow model mimics the target network, emulating the target model's differences in prediction probabilities for samples in and out of its dataset. For this attack, only one shadow model is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize models\n",
    "\n",
    "n_classes = valid_sequence_test_shadow.num_speakers\n",
    "print('n shadow speakers',n_classes)\n",
    "\n",
    "#NN\n",
    "shadow_net = shadow_net_type(n_classes).to(device)\n",
    "shadow_net.apply(models.weights_init)\n",
    "\n",
    "shadow_loss = nn.CrossEntropyLoss()\n",
    "shadow_optim = optim.Adam(shadow_net.parameters(), lr=lr)\n",
    "shadow_epochs = 1 #n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Attack Model\n",
    "A binary classifier to determine membership. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Attack the network: \n",
    "\n",
    "attack_net_nn = models.mlleaks_mlp(n_in=k).to(device)\n",
    "attack_loss = nn.BCEWithLogitsLoss()\n",
    "attack_optim_nn= optim.Adam(attack_net_nn.parameters(), lr=lr)\n",
    "n_epochs_attack = 1\n",
    "\n",
    "df_pr = cc.ml_leaks1(target=target_net, shadow_model = shadow_net, attacker_model = attack_net_nn,\n",
    "            target_in_loader = test_loader, target_out_loader = target_test_loader,\n",
    "            shadow_train_loader = shadow_train_loader, shadow_out_loader=shadow_test_loader,\n",
    "            shadow_optim = shadow_optim, attack_optim = attack_optim_nn, \n",
    "            shadow_criterion = shadow_loss, attack_criterion = attack_loss, \n",
    "            shadow_epochs = shadow_epochs, attack_epochs = n_epochs_attack, retrain = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ascertain best results\n",
    "\n",
    "df.at[df_idx,'Precision'] = round(df_pr[df_pr['Accuracy']==df_pr['Accuracy'].max()].Precision.values[0],4)\n",
    "df.at[df_idx,'Recall'] = round(df_pr[df_pr['Accuracy']==df_pr['Accuracy'].max()].Recall.values[0],4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(summary_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Attack Nets\n",
    "How well the trained attack models classify a sample as in or out of a target model's training dataset, and how performance is affected by target hyperparameters and which models attack which targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['# speakers'] =df['# speakers'].astype(float)\n",
    "df['Training epochs'] =df['Training epochs'].astype(float)\n",
    "df['Attack type'] =df['Attack type'].astype(float)\n",
    "\n",
    "#style table\n",
    "import seaborn as sns\n",
    "\n",
    "cg = sns.light_palette(\"green\", as_cmap=True)\n",
    "cm = sns.light_palette(\"magenta\", as_cmap=True)\n",
    "bl = sns.light_palette(\"blue\", as_cmap=True)\n",
    "orr = sns.light_palette(\"orange\", as_cmap=True)\n",
    "gr = sns.light_palette(\"gray\", as_cmap=True)\n",
    "\n",
    "# df.style.bar(subset=['Train accuracy', 'Test accuracy'], align='mid', color=['#d65f5f', '#5fba7d'])\n",
    "s = df.style.\\\n",
    "    background_gradient(cmap=cg,subset=['Train accuracy', 'Test accuracy']).\\\n",
    "    background_gradient(cmap=bl,subset=['Precision', 'Recall']).\\\n",
    "    background_gradient(cmap=orr,subset=['Training epochs']).\\\n",
    "    background_gradient(cmap=gr,subset=['Attack type']).\\\n",
    "    background_gradient(cmap=cm,subset=['# speakers']).\\\n",
    "    format({\"Train accuracy\": \"{:.2%}\",\"Test accuracy\": \"{:.2%}\"}).\\\n",
    "    hide_index().\\\n",
    "    set_properties(**{'font-size': \"16pt\",'column-size':\"24pt\",'width': '100px'})\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old, for reference for now:\n",
    "\n",
    "# do this for 10 & 100 speakers\n",
    "# .2 S & 3 S\n",
    "#sufficient training and over-training\n",
    "\n",
    "#manual data: \n",
    "\n",
    "#Attack 1:\n",
    "df.loc[len(df)] = ['MFCC',25,69.0,.9994,.9632,1,0.89,0.90] \n",
    "df.loc[len(df)] = ['MFCC',25,255.0,.9961,.9443,1,0.88,0.91] \n",
    "df.loc[len(df)] = ['STFT',25,69.0,0.9989,0.9451,1,0.89,0.92] \n",
    "df.loc[len(df)] = ['STFT',25,255.0,0.9958,0.9181,1,0.85,0.86] \n",
    "\n",
    "#Attack 3 w/max data: \n",
    "df.loc[len(df)] = ['STFT',25,139.0,.9985,.9073,3,.81,.90] \n",
    "df.loc[len(df)] = ['STFT',50,511.0,.9942,.9057,3,.84,.87] \n",
    "df.loc[len(df)] = ['MFCC',25,139.0,.9969,.9136,3,.82,.92] \n",
    "df.loc[len(df)] = ['MFCC',25,511.0,.9960,.9321,3,0.83,0.93]\n",
    "\n",
    "\n",
    "#Attack 3 on Attack1 models:\n",
    "df.loc[len(df)] = ['MFCC',25,69.0,.9994,.9632,3,0.84,0.95] \n",
    "df.loc[len(df)] = ['MFCC',25,255.0,.9961,.9443,3,0.84,0.94] \n",
    "df.loc[len(df)] = ['STFT',25,69.0,0.9989,0.9451,3,0.81,0.97] \n",
    "df.loc[len(df)] = ['STFT',25,255.0,0.9958,0.9181,3,0.81,0.90] \n",
    "\n",
    "df['# speakers'] =df['# speakers'].astype(float)\n",
    "df['Training epochs'] =df['Training epochs'].astype(float)\n",
    "df['Attack type'] =df['Attack type'].astype(float)\n",
    "\n",
    "#style table\n",
    "import seaborn as sns\n",
    "\n",
    "cg = sns.light_palette(\"green\", as_cmap=True)\n",
    "cm = sns.light_palette(\"magenta\", as_cmap=True)\n",
    "bl = sns.light_palette(\"blue\", as_cmap=True)\n",
    "orr = sns.light_palette(\"orange\", as_cmap=True)\n",
    "gr = sns.light_palette(\"gray\", as_cmap=True)\n",
    "\n",
    "# df.style.bar(subset=['Train accuracy', 'Test accuracy'], align='mid', color=['#d65f5f', '#5fba7d'])\n",
    "s = df.style.\\\n",
    "    background_gradient(cmap=cg,subset=['Train accuracy', 'Test accuracy']).\\\n",
    "    background_gradient(cmap=bl,subset=['Precision', 'Recall']).\\\n",
    "    background_gradient(cmap=orr,subset=['Training epochs']).\\\n",
    "    background_gradient(cmap=gr,subset=['Attack type']).\\\n",
    "    background_gradient(cmap=cm,subset=['# speakers']).\\\n",
    "    format({\"Train accuracy\": \"{:.2%}\",\"Test accuracy\": \"{:.2%}\"}).\\\n",
    "    hide_index().\\\n",
    "    set_properties(**{'font-size': \"16pt\",'column-size':\"24pt\",'width': '100px'})\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
