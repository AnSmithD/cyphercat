{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.5 (default, Jul  6 2018, 19:12:46) \n",
      "[GCC 5.4.0 20160609]\n",
      "Pytorch: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.insert(0, '../../../Utils/')\n",
    "sys.path.insert(0, '../../../')\n",
    "import cyphercat as cc\n",
    "\n",
    "import models\n",
    "from train import *\n",
    "from metrics import * \n",
    "from SVC_Utils import *\n",
    "\n",
    "#audio\n",
    "import librosa as libr\n",
    "\n",
    "print(\"Python: %s\" % sys.version)\n",
    "print(\"Pytorch: %s\" % torch.__version__)\n",
    "\n",
    "# determine device to run network on (runs on gpu if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.001\n",
    "k = 3\n",
    "\n",
    "pretrained = False #run this with networks that have already been trained\n",
    "\n",
    "transform_type = 'SFTF' #either STFT or MFCC  \n",
    "\n",
    "data = 'Libri' #'Libri' or 'VOiCES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speech preprocessing\n",
    "\n",
    "class tensorToMFCC:\n",
    "    def __call__(self, y):\n",
    "#         y = y.numpy()\n",
    "        dims = y.shape\n",
    "        y = libr.feature.melspectrogram(np.reshape(y, (dims[1],)), 16000, n_mels=number_of_mels,\n",
    "                               fmax=8000)\n",
    "        y = libr.feature.mfcc(S = libr.power_to_db(y))\n",
    "        y = torch.from_numpy(y)                           \n",
    "        return y.float()\n",
    "\n",
    "class STFT:\n",
    "    def __call__(self,y):\n",
    "        dims = y.shape\n",
    "        y = np.abs(libr.core.stft(np.reshape(y, (dims[1],))))\n",
    "        y = torch.from_numpy(y).permute(1,0)\n",
    "        return y.float()\n",
    "  \n",
    "if transform_type == 'SFTF':\n",
    "    target_net_type = cc.ft_cnn_classifer\n",
    "    shadow_net_type = cc.ft_cnn_classifer\n",
    "    in_size = 94# 20 forMFCC,  94 for STFT\n",
    "    transform  = STFT() ## STFT or MFCC\n",
    "elif transform_type == 'MFCC':\n",
    "    transform  = tensorToMFCC()\n",
    "    target_net_type = cc.MFCC_cnn_classifier\n",
    "    shadow_net_type = cc.MFCC_cnn_classifier\n",
    "    in_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seconds = 3\n",
    "n_epochs = 15\n",
    "sampling_rate = 16000\n",
    "number_of_mels =128\n",
    "lr = 0.001\n",
    "\n",
    "# attacking means data for a target & shadow network.\n",
    "# This will also split \"out data\" from totally different speakers -- data none of the \n",
    "# other networks have seen, for training & testing the attack network. This will be\n",
    "# an equivalent amount of data to the train split as defined about\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, './../../../Utils')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load audio data: VOiCES or LibriSpeech, & Split into valid sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading splits\n",
      "Initialising LibriSpeechDataset with minimum length = 3s and subset = train-clean-100\n",
      "Finished indexing data. 27949 usable files found.\n",
      "Found default splits, loading dataframe\n",
      "\t\t ---- Split 0 ---- \n",
      "\tUnique speakers \t Samples\n",
      "Male:\t\t 63\t\t 5539\n",
      "Female:\t\t 62\t\t 5573\n",
      "Total:\t\t 125\t\t 11112\n",
      "\t\t ---- Split 1 ---- \n",
      "\tUnique speakers \t Samples\n",
      "Male:\t\t 63\t\t 1414\n",
      "Female:\t\t 62\t\t 1427\n",
      "Total:\t\t 125\t\t 2841\n",
      "\t\t ---- Split 2 ---- \n",
      "\tUnique speakers \t Samples\n",
      "Male:\t\t 63\t\t 3471\n",
      "Female:\t\t 63\t\t 3490\n",
      "Total:\t\t 126\t\t 6961\n",
      "\t\t ---- Split 3 ---- \n",
      "\tUnique speakers \t Samples\n",
      "Male:\t\t 63\t\t 3510\n",
      "Female:\t\t 63\t\t 3525\n",
      "Total:\t\t 126\t\t 7035\n",
      "\t\t ---- Split 4 ---- \n",
      "\tUnique speakers \t Samples\n",
      "Male:\t\t 16\t\t 1414\n",
      "Female:\t\t 16\t\t 1427\n",
      "Total:\t\t 32\t\t 2841\n",
      "Finished splitting data.\n",
      "Initializing dataset\n",
      "Succesfully loaded libri-speech\n"
     ]
    }
   ],
   "source": [
    "print('Loading splits')\n",
    "\n",
    "if data == 'Libri':\n",
    "    dfs = cc.Libri_preload_and_split()\n",
    "    print('Initializing dataset')\n",
    "    valid_sequence_train = cc.LibriSpeechDataset(df=dfs[0], transform = transform)\n",
    "    valid_sequence_test = cc.LibriSpeechDataset(df=dfs[1], transform = transform)\n",
    "    valid_sequence_train_shadow = cc.LibriSpeechDataset(df=dfs[2], transform = transform)\n",
    "    valid_sequence_test_shadow = cc.LibriSpeechDataset(df=dfs[3], transform = transform)\n",
    "    valid_sequence_in = cc.LibriSpeechDataset(df=dfs[4], transform = transform)\n",
    "\n",
    "    print('Succesfully loaded libri-speech')\n",
    "elif data == 'VOiCES':\n",
    "    print('Loading splits')\n",
    "    dfs = cc.Voices_preload_and_split()\n",
    "    print('Initializing dataset')\n",
    "    valid_sequence_train = cc.Voices_dataset(df=dfs[0], transform = transform)\n",
    "    valid_sequence_test = cc.Voices_dataset(df=dfs[1], transform = transform)\n",
    "    valid_sequence_train_shadow = cc.Voices_dataset(df=dfs[2], transform = transform)\n",
    "    valid_sequence_test_shadow = cc.Voices_dataset(df=dfs[3], transform = transform)\n",
    "    valid_sequence_in = cc.Voices_dataset(df=dfs[4], transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Loaders for data for target model & shadow model \n",
    "target_train_loader = DataLoader(valid_sequence_train,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8,\n",
    "                    drop_last = True\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "\n",
    "target_test_loader = DataLoader(valid_sequence_test,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_train_loader = DataLoader(valid_sequence_train_shadow,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8,\n",
    "                    drop_last = True\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "\n",
    "shadow_test_loader = DataLoader(valid_sequence_test_shadow,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "\n",
    "#To do: adjust these to new data splits\n",
    "# target_out_loader = DataLoader(valid_sequence_out,\n",
    "#                       batch_size=batch_size,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=8,\n",
    "#                     drop_last = True\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "\n",
    "\n",
    "# shadow_out_loader = DataLoader(valid_sequence_out_shadow,\n",
    "#                       batch_size=batch_size,\n",
    "#                       shuffle=True,\n",
    "#                       num_workers=8,\n",
    "#                     drop_last = True\n",
    "#                      # pin_memory=True # CUDA only\n",
    "#                      )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table with summary\n",
    "\n",
    "# To do: extract accuracy from train/eval funcs and automatically add to table\n",
    "summary_file = 'summary.pkl'\n",
    "columns = ['Transform','Training epochs', '# speakers','Train accuracy', 'Test accuracy', 'Attack type', 'Precision','Recall']\n",
    "\n",
    "try:\n",
    "    df = pd.read_pickle(summary_file)\n",
    "\n",
    "except:\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "df_idx = len(df)\n",
    "\n",
    "#set a bunch of known values\n",
    "df.at[df_idx,'Transform'] =transform_type\n",
    "df.at[df_idx,'Training epochs'] = n_epochs\n",
    "df.at[df_idx,'Attack type'] = 'mlleaks1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Targets\n",
    "The model being attacked; if network, architecture can differ from that of shadow network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125  speakers\n"
     ]
    }
   ],
   "source": [
    "#Initialize NN\n",
    "\n",
    "#in_size defined above\n",
    "n_hidden = 512\n",
    "n_classes = valid_sequence_test.num_speakers\n",
    "print(n_classes,' speakers')\n",
    "df.at[df_idx,'# speakers']=n_classes\n",
    "\n",
    "\n",
    "target_net = target_net_type(n_classes).to(device)\n",
    "target_net.apply(models.weights_init)\n",
    "\n",
    "target_loss = nn.CrossEntropyLoss()\n",
    "target_optim = optim.Adam(target_net.parameters(), lr=lr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/15]\n",
      "Training:\n",
      "\n",
      "Accuracy = 66.52 %%\n",
      "\n",
      "\n",
      "Test:\n",
      "\n",
      "Accuracy = 57.87 %%\n",
      "\n",
      "\n",
      "[1/15]\n",
      "Training:\n",
      "\n",
      "Accuracy = 84.74 %%\n",
      "\n",
      "\n",
      "Test:\n",
      "\n",
      "Accuracy = 74.23 %%\n",
      "\n",
      "\n",
      "[2/15]\n",
      "Training:\n"
     ]
    }
   ],
   "source": [
    "#file name for this set of hyperparameters\n",
    "fn = 'model_weights/CNN_voice_classifier'+data+'_target_'+transform_type+str(n_epochs-1)+'.pth'\n",
    "\n",
    "#Train NN\n",
    "if not pretrained:\n",
    "    train_accuracy, test_accuracy = cc.train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs, verbose = False) \n",
    "    df.at[df_idx,'Train accuracy'] =train_accuracy\n",
    "    df.at[df_idx,'Test accuracy'] = test_accuracy\n",
    "    cc.save_checkpoint(model = target_net, optimizer = target_optim,\n",
    "                           epoch = n_epochs-1, data_descriptor = data, \n",
    "                           accuracy = [train_accuracy, test_accuracy],\n",
    "                           filename = fn)\n",
    "    \n",
    "else:\n",
    "    cc.load_checkpoint(model = target_net, optimizer = target_optim, checkpoint = fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Shadow Model\n",
    "Shadow model mimics the target network, emulating the target model's differences in prediction probabilities for samples in and out of its dataset. For this attack, only one shadow model is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize models\n",
    "\n",
    "n_classes = valid_sequence_test_shadow.num_speakers\n",
    "print('n shadow speakers',n_classes)\n",
    "\n",
    "#NN\n",
    "shadow_net = shadow_net_type(n_classes).to(device)\n",
    "shadow_net.apply(models.weights_init)\n",
    "\n",
    "shadow_loss = nn.CrossEntropyLoss()\n",
    "shadow_optim = optim.Adam(shadow_net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train NN\n",
    "fn = 'model_weights/CNN_voice_classifier'+data+'_shadow_'+transform_type+str(n_epochs-1)+'.pth'\n",
    "\n",
    "if not pretrained:\n",
    "    train_accuracy, test_accuracy = train(shadow_net, shadow_train_loader, shadow_test_loader, shadow_optim, shadow_loss, n_epochs, verbose = False)\n",
    "\n",
    "    models.save_checkpoint(model = shadow_net, optimizer = shadow_optim,\n",
    "                           epoch = n_epochs-1, data_descriptor = data, \n",
    "                           accuracy = [train_accuracy, test_accuracy],\n",
    "                           filename = fn)\n",
    "else:\n",
    "    load_checkpoint(model = shadow_net, optimizer = shadow_optim, checkpoint = fnm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Attack Model\n",
    "A binary classifier to determine membership. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates two attack nets for comparison.\n",
    "\n",
    "# attack_net_svm = models.mlleaks_mlp(n_in=k).to(device)\n",
    "# attack_net_svm.apply(models.weights_init)\n",
    "\n",
    "# Attack the network: \n",
    "\n",
    "\n",
    "attack_net_nn = cc.ml_leaks1()\n",
    "attack_net_nn.apply(models.weights_init)\n",
    "\n",
    "attack_loss = nn.BCEWithLogitsLoss() #this one works\n",
    "# attack_loss = nn.BCELoss() # this one doesn't work \n",
    "# attack_optim_svm= optim.Adam(attack_net_svm.parameters(), lr=lr)\n",
    "attack_optim_nn= optim.Adam(attack_net_nn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains SVM attack model\n",
    "# train_attacker(attack_net_svm, shadow_svm, shadow_train_loader, shadow_out_loader, attack_optim_svm, attack_loss, n_epochs=2, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains NN attack model\n",
    "attack_loss = nn.BCEWithLogitsLoss()\n",
    "n_epochs_attack = 50\n",
    "\n",
    "if not pretrained:\n",
    "    train_attacker(attack_net_nn, shadow_net, shadow_train_loader, shadow_out_loader, attack_optim_nn, \n",
    "                   attack_loss, n_epochs=n_epochs_attack, k=k)\n",
    "    \n",
    "else:\n",
    "    fnm = 'model_weights/CNN_voice_classifier'+str(data_set)+'_attack_'+transform_type+str(24)+'.pth'\n",
    "#     fnm = 'model_weights/CNN_voice_classifier100_target_'+str(epoch)+'.pth'\n",
    "    load_checkpoint(model = attack_net_nn, optimizer = attack_optim_nn, checkpoint = fnm)\n",
    "#     chpt = torch.load(fnm)\n",
    "#     attack_net_nn.load_state_dict(chpt['state_dict'])\n",
    "\n",
    "#original:\n",
    "fn = 'model_weights/CNN_voice_classifier'+str(data_set)+'_shadow_'+transform_type+str(n_epochs-1)+'.pth'\n",
    "\n",
    "if not pretrained:\n",
    "\n",
    "    models.save_checkpoint(model = shadow_net, optimizer = shadow_optim,\n",
    "                           epoch = n_epochs-1, data_descriptor = all_data, \n",
    "                           accuracy = [train_accuracy, test_accuracy],\n",
    "                           filename = fn)# train_attacker(attack_net_nn, shadow_net, shadow_train_loader, shadow_out_loader, attack_optim_nn, attack_loss, n_epochs=50, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'model_weights/CNN_voice_classifier'+str(data_set)+'_attack_'+transform_type+str(n_epochs-1)+'.pth'\n",
    "\n",
    "if not pretrained:\n",
    "\n",
    "    models.save_checkpoint(model = attack_net_nn, optimizer = attack_optim_nn,\n",
    "                           epoch = n_epochs-1, data_descriptor = all_data, \n",
    "                           accuracy = [train_accuracy, test_accuracy],\n",
    "                           filename = fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Attack Nets\n",
    "How well the trained attack models classify a sample as in or out of a target model's training dataset, and how performance is affected by target hyperparameters and which models attack which targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#attack net trained on svm shadow model on svm target, C=10, gamma=.1\n",
    "# eval_attack_net(attack_net_svm, gen_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack net trained on svm shadow model on svm target, C=1, gamma=.01\n",
    "# eval_attack_net(attack_net_svm, maxacc_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#attack net trained on nn shadow model on nn target\n",
    "eval_attack_net(attack_net_nn, target_net, target_train_loader, target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack net trained on nn shadow model on svm target, C=10, gamma=.1\n",
    "# eval_attack_net(attack_net_nn, gen_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do this for 10 & 100 speakers\n",
    "# .2 S & 3 S\n",
    "#sufficient training and over-training\n",
    "\n",
    "#manual data: \n",
    "\n",
    "#Attack 1:\n",
    "df.loc[len(df)] = ['MFCC',25,69.0,.9994,.9632,1,0.89,0.90] \n",
    "df.loc[len(df)] = ['MFCC',25,255.0,.9961,.9443,1,0.88,0.91] \n",
    "df.loc[len(df)] = ['STFT',25,69.0,0.9989,0.9451,1,0.89,0.92] \n",
    "df.loc[len(df)] = ['STFT',25,255.0,0.9958,0.9181,1,0.85,0.86] \n",
    "\n",
    "#Attack 3 w/max data: \n",
    "df.loc[len(df)] = ['STFT',25,139.0,.9985,.9073,3,.81,.90] \n",
    "df.loc[len(df)] = ['STFT',50,511.0,.9942,.9057,3,.84,.87] \n",
    "df.loc[len(df)] = ['MFCC',25,139.0,.9969,.9136,3,.82,.92] \n",
    "df.loc[len(df)] = ['MFCC',25,511.0,.9960,.9321,3,0.83,0.93]\n",
    "\n",
    "\n",
    "#Attack 3 on Attack1 models:\n",
    "df.loc[len(df)] = ['MFCC',25,69.0,.9994,.9632,3,0.84,0.95] \n",
    "df.loc[len(df)] = ['MFCC',25,255.0,.9961,.9443,3,0.84,0.94] \n",
    "df.loc[len(df)] = ['STFT',25,69.0,0.9989,0.9451,3,0.81,0.97] \n",
    "df.loc[len(df)] = ['STFT',25,255.0,0.9958,0.9181,3,0.81,0.90] \n",
    "\n",
    "df['Training epochs'] =df['Training epochs'].astype(float)\n",
    "df['Attack type'] =df['Attack type'].astype(float)\n",
    "\n",
    "#style table\n",
    "import seaborn as sns\n",
    "\n",
    "cg = sns.light_palette(\"green\", as_cmap=True)\n",
    "cm = sns.light_palette(\"magenta\", as_cmap=True)\n",
    "bl = sns.light_palette(\"blue\", as_cmap=True)\n",
    "orr = sns.light_palette(\"orange\", as_cmap=True)\n",
    "gr = sns.light_palette(\"gray\", as_cmap=True)\n",
    "\n",
    "# df.style.bar(subset=['Train accuracy', 'Test accuracy'], align='mid', color=['#d65f5f', '#5fba7d'])\n",
    "s = df.style.\\\n",
    "    background_gradient(cmap=cg,subset=['Train accuracy', 'Test accuracy']).\\\n",
    "    background_gradient(cmap=bl,subset=['Precision', 'Recall']).\\\n",
    "    background_gradient(cmap=orr,subset=['Training epochs']).\\\n",
    "    background_gradient(cmap=gr,subset=['Attack type']).\\\n",
    "    background_gradient(cmap=cm,subset=['# speakers']).\\\n",
    "    format({\"Train accuracy\": \"{:.2%}\",\"Test accuracy\": \"{:.2%}\"}).\\\n",
    "    hide_index().\\\n",
    "    set_properties(**{'font-size': \"16pt\",'column-size':\"24pt\",'width': '100px'})\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
