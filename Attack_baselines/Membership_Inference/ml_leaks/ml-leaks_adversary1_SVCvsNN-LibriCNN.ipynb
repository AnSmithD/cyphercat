{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.5 (default, Jul  6 2018, 19:12:46) \n",
      "[GCC 5.4.0 20160609]\n",
      "Pytorch: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.insert(0, '../../../Utils/')\n",
    "\n",
    "import models\n",
    "from train import *\n",
    "from metrics import * \n",
    "from SVC_Utils import *\n",
    "\n",
    "#audio\n",
    "import librosa as libr\n",
    "\n",
    "print(\"Python: %s\" % sys.version)\n",
    "print(\"Pytorch: %s\" % torch.__version__)\n",
    "\n",
    "# determine device to run network on (runs on gpu if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "k = 3\n",
    "\n",
    "#changed from ML_Leaks to Michael's CNN\n",
    "target_net_type = models.CNN_classifier\n",
    "shadow_net_type = models.CNN_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LibriSpeech data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising LibriSpeechDataset with minimum length = 3s and subsets = ['train-clean-360']\n",
      "Finished indexing data. 101703 usable files found.\n",
      "Finished splitting data.\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "n_seconds = 3\n",
    "n_epochs = 25\n",
    "sampling_rate = 16000\n",
    "number_of_mels =128\n",
    "all_data = ['train-clean-100']\n",
    "lr = 0.001\n",
    "\n",
    "### Speech preprocessing\n",
    "\n",
    "class tensorToMFCC:\n",
    "    def __call__(self, y):\n",
    "#         y = y.numpy()\n",
    "        dims = y.shape\n",
    "        y = libr.feature.melspectrogram(np.reshape(y, (dims[1],)), 16000, n_mels=number_of_mels,\n",
    "                               fmax=8000)\n",
    "        y = libr.feature.mfcc(S = libr.power_to_db(y))\n",
    "        y = torch.from_numpy(y)                           \n",
    "        return y.float()\n",
    "    \n",
    "transform  = tensorToMFCC()\n",
    "\n",
    "### Data set\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, './../../../Utils')\n",
    "from datasets import LibriSpeechDataset\n",
    "from datasets import Libri_preload_and_split\n",
    "\n",
    "\n",
    "\n",
    "path = './../../../Classification_baselines/LibriSpeech/data/'\n",
    "\n",
    "splits = [0.4, 0.4, 0.2] #input fraction of data you want partitioned\n",
    "attacking = True\n",
    "\n",
    "if sum(splits) != 1:\n",
    "    print('error: splits do not sum to 1.')\n",
    "\n",
    "#Splits data into 2 sets of speakers for target & shadow network, into above defined train:test splits\n",
    "dfs = Libri_preload_and_split(path,all_data,n_seconds,pad=False,cache=True,splits=splits, attacking = attacking)  \n",
    "\n",
    "#target train & test\n",
    "valid_sequence_train = LibriSpeechDataset(path, df = dfs[0], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_out = LibriSpeechDataset(path, df = dfs[1], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_test = LibriSpeechDataset(path, df = dfs[2], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "#shadow train & test\n",
    "valid_sequence_train_shadow = LibriSpeechDataset(path, df = dfs[3], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_out_shadow = LibriSpeechDataset(path, df = dfs[4], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_test_shadow = LibriSpeechDataset(path, df = dfs[5], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders for data for target model & shadow model \n",
    "target_train_loader = DataLoader(valid_sequence_train,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "target_out_loader = DataLoader(valid_sequence_out,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "target_test_loader = DataLoader(valid_sequence_test,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_train_loader = DataLoader(valid_sequence_train_shadow,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_out_loader = DataLoader(valid_sequence_out_shadow,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_test_loader = DataLoader(valid_sequence_test_shadow,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define series of transforms to pre process images \n",
    "# train_transform = torchvision.transforms.Compose([\n",
    "#     #torchvision.transforms.Pad(2),\n",
    "    \n",
    "\n",
    "#     #torchvision.transforms.RandomRotation(10),\n",
    "#     #torchvision.transforms.RandomHorizontalFlip(),\n",
    "#     #torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    \n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#     torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# ])\n",
    "\n",
    "# test_transform = torchvision.transforms.Compose([\n",
    "#     #torchvision.transforms.Pad(2),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#     torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# ])\n",
    "    \n",
    "\n",
    "# classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "\n",
    "# # load training set \n",
    "# cifar10_trainset = torchvision.datasets.CIFAR10('../../../Datasets/', train=True, transform=train_transform, download=True)\n",
    "# cifar10_trainloader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# #for svms\n",
    "# sv_cifar10_trainset = torchvision.datasets.CIFAR10('../../../Datasets/', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "# sv_cifar10_trainloader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# # load test set \n",
    "# cifar10_testset = torchvision.datasets.CIFAR10('../../../Datasets/', train=False, transform=test_transform, download=True)\n",
    "# cifar10_testloader = torch.utils.data.DataLoader(cifar10_testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# sv_cifar10_testset = torchvision.datasets.CIFAR10('../../../Datasets/', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "# sv_cifar10_testloader = torch.utils.data.DataLoader(sv_cifar10_testset, batch_size=cifar10_testset.__len__(), shuffle=True, num_workers=2)\n",
    "\n",
    "# # helper function to unnormalize and plot image \n",
    "# def imshow(img):\n",
    "#     img = np.array(img)\n",
    "#     img = img / 2 + 0.5\n",
    "#     img = np.moveaxis(img, 0, -1)\n",
    "#     plt.imshow(img)\n",
    "    \n",
    "# # display sample from dataset \n",
    "# imgs,labels = iter(cifar10_trainloader).next()\n",
    "# imshow(torchvision.utils.make_grid(imgs))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates two non-overlapping subsets of CIFAR10 to train the shadow and target models. We assume the attacker \n",
    "# # has access to data that is similar to but not the same as the data used to train the target.\n",
    "\n",
    "# total_size = len(cifar10_trainset)\n",
    "# split1 = total_size // 4\n",
    "# split2 = split1*2\n",
    "# split3 = split1*3\n",
    "\n",
    "# indices = list(range(total_size))\n",
    "\n",
    "# shadow_train_idx = indices[:split1]\n",
    "# shadow_out_idx = indices[split1:split2]\n",
    "# target_train_idx = indices[split2:split3]\n",
    "# target_out_idx = indices[split3:]\n",
    "\n",
    "\n",
    "# shadow_train_sampler = SubsetRandomSampler(shadow_train_idx)\n",
    "# shadow_out_sampler = SubsetRandomSampler(shadow_out_idx)\n",
    "# target_train_sampler = SubsetRandomSampler(target_train_idx)\n",
    "# target_out_sampler = SubsetRandomSampler(target_out_idx)\n",
    "\n",
    "# shadow_train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, sampler=shadow_train_sampler, num_workers=1)\n",
    "# shadow_out_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, sampler=shadow_out_sampler, num_workers=1)\n",
    "\n",
    "# #To fit shadow SVM\n",
    "# sv_shadow_train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=shadow_train_sampler.__len__(), sampler=shadow_train_sampler, num_workers=1)\n",
    "\n",
    "# #attack_train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=32, sampler=shadow_train_sampler, num_workers=1)\n",
    "\n",
    "# #attack_out_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=32, sampler=shadow_out_sampler, num_workers=1)\n",
    "# target_train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, sampler=target_train_sampler, num_workers=1)\n",
    "# target_out_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, sampler=target_out_sampler, num_workers=1)\n",
    "\n",
    "# #for svms\n",
    "# sv_target_train_loader = torch.utils.data.DataLoader(sv_cifar10_trainset, batch_size=batch_size, sampler=target_train_sampler, num_workers=1)\n",
    "# sv_target_out_loader = torch.utils.data.DataLoader(sv_cifar10_trainset, batch_size=batch_size, sampler=target_out_sampler, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Targets\n",
    "The model being attacked; if network, architecture can differ from that of shadow network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n"
     ]
    }
   ],
   "source": [
    "#Initialize NN\n",
    "\n",
    "# change target to this? \n",
    "#classifier = CNN_classifier(20, 512, valid_sequence_test.num_speakers)\n",
    "\n",
    "in_size = 20\n",
    "n_hidden = 512\n",
    "n_classes = valid_sequence_test.num_speakers\n",
    "print(n_classes)\n",
    "\n",
    "target_net = target_net_type(in_size,n_hidden,n_classes).to(device)\n",
    "target_net.apply(models.weights_init)\n",
    "\n",
    "target_loss = nn.CrossEntropyLoss()\n",
    "target_optim = optim.Adam(target_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"./../../../Utils/datasets.py\", line 240, in __getitem__\n    instance, samplerate = sf.read(self.datasetid_to_filepath[index])\n  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 257, in read\n    subtype, endian, format, closefd) as f:\n  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 627, in __init__\n    self._file = self._open(file, mode_int, closefd)\n  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1182, in _open\n    \"Error opening {0!r}: \".format(self.name))\n  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1355, in _error_check\n    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))\nRuntimeError: Error opening 'data//LibriSpeech/train-clean-360/2589/22574/2589-22574-0008.flac': System error.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-30064666d3d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#blend:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_test_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#classes = range(valid_sequence_test.num_speakers),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/fs4/home/nlopatina/cyphercat/Utils/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data_loader, test_loader, optimizer, criterion, n_epochs, classes, verbose)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"./../../../Utils/datasets.py\", line 240, in __getitem__\n    instance, samplerate = sf.read(self.datasetid_to_filepath[index])\n  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 257, in read\n    subtype, endian, format, closefd) as f:\n  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 627, in __init__\n    self._file = self._open(file, mode_int, closefd)\n  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1182, in _open\n    \"Error opening {0!r}: \".format(self.name))\n  File \"/home/nlopatina/.local/lib/python3.6/site-packages/soundfile.py\", line 1355, in _error_check\n    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))\nRuntimeError: Error opening 'data//LibriSpeech/train-clean-360/2589/22574/2589-22574-0008.flac': System error.\n"
     ]
    }
   ],
   "source": [
    "#Train NN\n",
    "#from ML:\n",
    "# train(classifier, train_loader, test_loader, optimizer, criterion, 50, verbose = False)\n",
    "\n",
    "#blend:\n",
    "train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs, verbose = False) #classes = range(valid_sequence_test.num_speakers),\n",
    "\n",
    "\n",
    "#here:\n",
    "# train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs)#, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 24\n",
    "save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': 'CNN_voice_classifier',\n",
    "            'state_dict': target_net.state_dict(),\n",
    "            'optimizer' : target_optim.state_dict(),\n",
    "        }, False, filename = 'model_weights/CNN_voice_classifier360_target_'+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold off on adapting SVM for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize SVM\n",
    "\n",
    "# # #The stored baseline SVM was fit using all of CIFAR10 training data. To attack for membership inference, use \n",
    "# # #images not in CIFAR10 training data, or fit new classifiers/run source code with subset of CIFAR10.\n",
    "\n",
    "# # '''\n",
    "# # dir='../../../Classification_baselines/CIFAR10'\n",
    "# # target_gen=load_svm(dir, gen=True)\n",
    "# # target_maxacc=load_svm(dir, gen=False)\n",
    "# # '''\n",
    "\n",
    "# # #Training example targets on loaded CIFAR10 target subset:\n",
    "\n",
    "# gen_svm=make_pipeline(PCA(n_components=180), MinMaxScaler(feature_range=(-1,1)), svm.SVC(C=10, gamma=.1, probability=True))\n",
    "# maxacc_svm=make_pipeline(PCA(n_components=180), MinMaxScaler(feature_range=(-1,1)), svm.SVC(C=1, gamma=.01, probability=True))\n",
    "\n",
    "# # sv_target_fit_loader = torch.utils.data.DataLoader(sv_cifar10_trainset, batch_size=target_train_sampler.__len__(), \n",
    "# #                                                    sampler=target_train_sampler, num_workers=1)\n",
    "\n",
    "\n",
    "# tin, tout=load(target_train_loader)\n",
    "\n",
    "# #Train SVM\n",
    "# gen_svm.fit(tin, tout)\n",
    "# maxacc_svm.fit(tin, tout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #evaluate SVM targets\n",
    "\n",
    "# classes = range(n_classes)\n",
    "# inp, outp=load(target_test_loader)\n",
    "\n",
    "# print('SVM A (C=', gen_svm.get_params(deep=True)['svc__C'], ', gamma= ',\n",
    "#       gen_svm.get_params(deep=True)['svc__gamma'], '): ')\n",
    "# class_acc(gen_svm.predict_proba(inp), outp, classes)\n",
    "\n",
    "# print('SVM B (C=', maxacc_svm.get_params(deep=True)['svc__C'], ', gamma= ',\n",
    "#       maxacc_svm.get_params(deep=True)['svc__gamma'], '): ')\n",
    "# class_acc(maxacc_svm.predict_proba(inp), outp, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Shadow Model\n",
    "Shadow model mimics the target network, emulating the target model's differences in prediction probabilities for samples in and out of its dataset. For this attack, only one shadow model is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize models\n",
    "\n",
    "n_classes = valid_sequence_test_shadow.num_speakers\n",
    "print(n_classes)\n",
    "\n",
    "#NN\n",
    "shadow_net = shadow_net_type(in_size,n_hidden,n_classes).to(device)\n",
    "shadow_net.apply(models.weights_init)\n",
    "\n",
    "shadow_loss = nn.CrossEntropyLoss()\n",
    "shadow_optim = optim.Adam(shadow_net.parameters(), lr=lr)\n",
    "\n",
    "#SVM\n",
    "# shadowinputs, shadowtargets=load(sv_shadow_train_loader)\n",
    "# shadow_svm=make_pipeline(PCA(n_components=180), MinMaxScaler(feature_range=(-1,1)), \n",
    "#                          svm.SVC(C=1, gamma=.1, probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(shadowinputs))\n",
    "# shadowtargets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train NN\n",
    "\n",
    "# below commented code is for comparison during debugging\n",
    "# target_net = target_net_type(in_size,n_hidden,n_classes).to(device)\n",
    "# target_net.apply(models.weights_init)\n",
    "\n",
    "# target_loss = nn.CrossEntropyLoss()\n",
    "# target_optim = optim.Adam(target_net.parameters(), lr=lr)\n",
    "\n",
    "# train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs, verbose = False) #classes = range(valid_sequence_test.num_speakers),\n",
    "\n",
    "\n",
    "train(shadow_net, shadow_train_loader, shadow_test_loader, shadow_optim, shadow_loss, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train SVM\n",
    "# shadow_svm.fit(shadowinputs, shadowtargets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Attack Model\n",
    "A binary classifier to determine membership. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates two attack nets for comparison.\n",
    "\n",
    "# attack_net_svm = models.mlleaks_mlp(n_in=k).to(device)\n",
    "# attack_net_svm.apply(models.weights_init)\n",
    "\n",
    "attack_net_nn = models.mlleaks_mlp(n_in=k).to(device)\n",
    "attack_net_nn.apply(models.weights_init)\n",
    "\n",
    "attack_loss = nn.BCEWithLogitsLoss() #this one works\n",
    "# attack_loss = nn.BCELoss() # this one doesn't work \n",
    "# attack_optim_svm= optim.Adam(attack_net_svm.parameters(), lr=lr)\n",
    "attack_optim_nn= optim.Adam(attack_net_nn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains SVM attack model\n",
    "# train_attacker(attack_net_svm, shadow_svm, shadow_train_loader, shadow_out_loader, attack_optim_svm, attack_loss, n_epochs=2, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains NN attack model\n",
    "#to do: change epochs to 50\n",
    "train_attacker(attack_net_nn, shadow_net, shadow_train_loader, shadow_out_loader, attack_optim_nn, \n",
    "               attack_loss, n_epochs=50, k=k)\n",
    "\n",
    "#original:\n",
    "# train_attacker(attack_net_nn, shadow_net, shadow_train_loader, shadow_out_loader, attack_optim_nn, attack_loss, n_epochs=50, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "epoch = 24\n",
    "save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': 'CNN_voice_classifier',\n",
    "            'state_dict': target_net.state_dict(),\n",
    "            'optimizer' : target_optim.state_dict(),\n",
    "        }, False, filename = 'model_weights/CNN_voice_classifier360_shadow_'+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Attack Nets\n",
    "How well the trained attack models classify a sample as in or out of a target model's training dataset, and how performance is affected by target hyperparameters and which models attack which targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#attack net trained on svm shadow model on svm target, C=10, gamma=.1\n",
    "# eval_attack_net(attack_net_svm, gen_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack net trained on svm shadow model on svm target, C=1, gamma=.01\n",
    "# eval_attack_net(attack_net_svm, maxacc_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#attack net trained on nn shadow model on nn target\n",
    "eval_attack_net(attack_net_nn, target_net, target_train_loader, target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack net trained on nn shadow model on svm target, C=10, gamma=.1\n",
    "# eval_attack_net(attack_net_nn, gen_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
