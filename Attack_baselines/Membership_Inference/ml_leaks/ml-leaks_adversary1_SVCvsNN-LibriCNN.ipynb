{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.5 (default, Jul  6 2018, 19:12:46) \n",
      "[GCC 5.4.0 20160609]\n",
      "Pytorch: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.insert(0, '../../../Utils/')\n",
    "\n",
    "import models\n",
    "from train import *\n",
    "from metrics import * \n",
    "from SVC_Utils import *\n",
    "\n",
    "#audio\n",
    "import librosa as libr\n",
    "\n",
    "print(\"Python: %s\" % sys.version)\n",
    "print(\"Pytorch: %s\" % torch.__version__)\n",
    "\n",
    "# determine device to run network on (runs on gpu if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "k = 3\n",
    "\n",
    "#changed from ML_Leaks to Michael's CNN\n",
    "target_net_type = models.CNN_classifier\n",
    "shadow_net_type = models.CNN_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LibriSpeech data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising LibriSpeechDataset with minimum length = 3s and subsets = ['train-clean-100']\n",
      "Finished indexing data. 27949 usable files found.\n",
      "Finished splitting data.\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "n_seconds = 3\n",
    "n_epochs = 2#25\n",
    "sampling_rate = 16000\n",
    "number_of_mels =128\n",
    "all_data = ['train-clean-100']\n",
    "lr = 0.001\n",
    "\n",
    "### Speech preprocessing\n",
    "\n",
    "class tensorToMFCC:\n",
    "    def __call__(self, y):\n",
    "#         y = y.numpy()\n",
    "        dims = y.shape\n",
    "        y = libr.feature.melspectrogram(np.reshape(y, (dims[1],)), 16000, n_mels=number_of_mels,\n",
    "                               fmax=8000)\n",
    "        y = libr.feature.mfcc(S = libr.power_to_db(y))\n",
    "        y = torch.from_numpy(y)                           \n",
    "        return y.float()\n",
    "    \n",
    "transform  = tensorToMFCC()\n",
    "\n",
    "### Data set\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, './../../../Utils')\n",
    "from datasets import LibriSpeechDataset\n",
    "from datasets import Libri_preload_and_split\n",
    "\n",
    "\n",
    "\n",
    "path = './../../../Classification_baselines/LibriSpeech/data'\n",
    "\n",
    "splits = [0.4, 0.4, 0.2] #input fraction of data you want partitioned\n",
    "attacking = True\n",
    "\n",
    "if sum(splits) != 1:\n",
    "    print('error: splits do not sum to 1.')\n",
    "\n",
    "#Splits data into 2 sets of speakers for target & shadow network, into above defined train:test splits\n",
    "dfs = Libri_preload_and_split(path,all_data,n_seconds,pad=False,cache=True,splits=splits, attacking = attacking)  \n",
    "\n",
    "#target train & test\n",
    "valid_sequence_train = LibriSpeechDataset(path, df = dfs[0], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_out = LibriSpeechDataset(path, df = dfs[1], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_test = LibriSpeechDataset(path, df = dfs[2], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "#shadow train & test\n",
    "valid_sequence_train_shadow = LibriSpeechDataset(path, df = dfs[3], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_out_shadow = LibriSpeechDataset(path, df = dfs[4], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_test_shadow = LibriSpeechDataset(path, df = dfs[5], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders for data for target model & shadow model \n",
    "target_train_loader = DataLoader(valid_sequence_train,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "target_out_loader = DataLoader(valid_sequence_out,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "target_test_loader = DataLoader(valid_sequence_test,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_train_loader = DataLoader(valid_sequence_train_shadow,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_out_loader = DataLoader(valid_sequence_out_shadow,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_test_loader = DataLoader(valid_sequence_test_shadow,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define series of transforms to pre process images \n",
    "# train_transform = torchvision.transforms.Compose([\n",
    "#     #torchvision.transforms.Pad(2),\n",
    "    \n",
    "\n",
    "#     #torchvision.transforms.RandomRotation(10),\n",
    "#     #torchvision.transforms.RandomHorizontalFlip(),\n",
    "#     #torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    \n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#     torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# ])\n",
    "\n",
    "# test_transform = torchvision.transforms.Compose([\n",
    "#     #torchvision.transforms.Pad(2),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#     torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# ])\n",
    "    \n",
    "\n",
    "# classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "\n",
    "# # load training set \n",
    "# cifar10_trainset = torchvision.datasets.CIFAR10('../../../Datasets/', train=True, transform=train_transform, download=True)\n",
    "# cifar10_trainloader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# #for svms\n",
    "# sv_cifar10_trainset = torchvision.datasets.CIFAR10('../../../Datasets/', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "# sv_cifar10_trainloader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# # load test set \n",
    "# cifar10_testset = torchvision.datasets.CIFAR10('../../../Datasets/', train=False, transform=test_transform, download=True)\n",
    "# cifar10_testloader = torch.utils.data.DataLoader(cifar10_testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# sv_cifar10_testset = torchvision.datasets.CIFAR10('../../../Datasets/', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "# sv_cifar10_testloader = torch.utils.data.DataLoader(sv_cifar10_testset, batch_size=cifar10_testset.__len__(), shuffle=True, num_workers=2)\n",
    "\n",
    "# # helper function to unnormalize and plot image \n",
    "# def imshow(img):\n",
    "#     img = np.array(img)\n",
    "#     img = img / 2 + 0.5\n",
    "#     img = np.moveaxis(img, 0, -1)\n",
    "#     plt.imshow(img)\n",
    "    \n",
    "# # display sample from dataset \n",
    "# imgs,labels = iter(cifar10_trainloader).next()\n",
    "# imshow(torchvision.utils.make_grid(imgs))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates two non-overlapping subsets of CIFAR10 to train the shadow and target models. We assume the attacker \n",
    "# # has access to data that is similar to but not the same as the data used to train the target.\n",
    "\n",
    "# total_size = len(cifar10_trainset)\n",
    "# split1 = total_size // 4\n",
    "# split2 = split1*2\n",
    "# split3 = split1*3\n",
    "\n",
    "# indices = list(range(total_size))\n",
    "\n",
    "# shadow_train_idx = indices[:split1]\n",
    "# shadow_out_idx = indices[split1:split2]\n",
    "# target_train_idx = indices[split2:split3]\n",
    "# target_out_idx = indices[split3:]\n",
    "\n",
    "\n",
    "# shadow_train_sampler = SubsetRandomSampler(shadow_train_idx)\n",
    "# shadow_out_sampler = SubsetRandomSampler(shadow_out_idx)\n",
    "# target_train_sampler = SubsetRandomSampler(target_train_idx)\n",
    "# target_out_sampler = SubsetRandomSampler(target_out_idx)\n",
    "\n",
    "# shadow_train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, sampler=shadow_train_sampler, num_workers=1)\n",
    "# shadow_out_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, sampler=shadow_out_sampler, num_workers=1)\n",
    "\n",
    "# #To fit shadow SVM\n",
    "# sv_shadow_train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=shadow_train_sampler.__len__(), sampler=shadow_train_sampler, num_workers=1)\n",
    "\n",
    "# #attack_train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=32, sampler=shadow_train_sampler, num_workers=1)\n",
    "\n",
    "# #attack_out_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=32, sampler=shadow_out_sampler, num_workers=1)\n",
    "# target_train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, sampler=target_train_sampler, num_workers=1)\n",
    "# target_out_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, sampler=target_out_sampler, num_workers=1)\n",
    "\n",
    "# #for svms\n",
    "# sv_target_train_loader = torch.utils.data.DataLoader(sv_cifar10_trainset, batch_size=batch_size, sampler=target_train_sampler, num_workers=1)\n",
    "# sv_target_out_loader = torch.utils.data.DataLoader(sv_cifar10_trainset, batch_size=batch_size, sampler=target_out_sampler, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Targets\n",
    "The model being attacked; if network, architecture can differ from that of shadow network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "#Initialize NN\n",
    "\n",
    "# change target to this? \n",
    "#classifier = CNN_classifier(20, 512, valid_sequence_test.num_speakers)\n",
    "\n",
    "in_size = 20\n",
    "n_hidden = 512\n",
    "n_classes = valid_sequence_test.num_speakers\n",
    "print(n_classes)\n",
    "\n",
    "target_net = target_net_type(in_size,n_hidden,n_classes).to(device)\n",
    "target_net.apply(models.weights_init)\n",
    "\n",
    "target_loss = nn.CrossEntropyLoss()\n",
    "target_optim = optim.Adam(target_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/2]\n",
      "Training:\n",
      "\n",
      "Accuracy = 72.86 %\n",
      "\n",
      "\n",
      "Test:\n",
      "\n",
      "Accuracy = 47.62 %\n",
      "\n",
      "\n",
      "[1/2]\n",
      "Training:\n",
      "\n",
      "Accuracy = 91.42 %\n",
      "\n",
      "\n",
      "Test:\n",
      "\n",
      "Accuracy = 61.09 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train NN\n",
    "#from ML:\n",
    "# train(classifier, train_loader, test_loader, optimizer, criterion, 50, verbose = False)\n",
    "\n",
    "#blend:\n",
    "train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs, verbose = False) #classes = range(valid_sequence_test.num_speakers),\n",
    "\n",
    "\n",
    "#here:\n",
    "# train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs)#, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 4\n",
    "save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': 'CNN_voice_classifier',\n",
    "            'state_dict': target_net.state_dict(),\n",
    "            'optimizer' : target_optim.state_dict(),\n",
    "        }, False, filename = 'model_weights/CNN_voice_classifier100_target_'+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold off on adapting SVM for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize SVM\n",
    "\n",
    "# # #The stored baseline SVM was fit using all of CIFAR10 training data. To attack for membership inference, use \n",
    "# # #images not in CIFAR10 training data, or fit new classifiers/run source code with subset of CIFAR10.\n",
    "\n",
    "# # '''\n",
    "# # dir='../../../Classification_baselines/CIFAR10'\n",
    "# # target_gen=load_svm(dir, gen=True)\n",
    "# # target_maxacc=load_svm(dir, gen=False)\n",
    "# # '''\n",
    "\n",
    "# # #Training example targets on loaded CIFAR10 target subset:\n",
    "\n",
    "# gen_svm=make_pipeline(PCA(n_components=180), MinMaxScaler(feature_range=(-1,1)), svm.SVC(C=10, gamma=.1, probability=True))\n",
    "# maxacc_svm=make_pipeline(PCA(n_components=180), MinMaxScaler(feature_range=(-1,1)), svm.SVC(C=1, gamma=.01, probability=True))\n",
    "\n",
    "# # sv_target_fit_loader = torch.utils.data.DataLoader(sv_cifar10_trainset, batch_size=target_train_sampler.__len__(), \n",
    "# #                                                    sampler=target_train_sampler, num_workers=1)\n",
    "\n",
    "\n",
    "# tin, tout=load(target_train_loader)\n",
    "\n",
    "# #Train SVM\n",
    "# gen_svm.fit(tin, tout)\n",
    "# maxacc_svm.fit(tin, tout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #evaluate SVM targets\n",
    "\n",
    "# classes = range(n_classes)\n",
    "# inp, outp=load(target_test_loader)\n",
    "\n",
    "# print('SVM A (C=', gen_svm.get_params(deep=True)['svc__C'], ', gamma= ',\n",
    "#       gen_svm.get_params(deep=True)['svc__gamma'], '): ')\n",
    "# class_acc(gen_svm.predict_proba(inp), outp, classes)\n",
    "\n",
    "# print('SVM B (C=', maxacc_svm.get_params(deep=True)['svc__C'], ', gamma= ',\n",
    "#       maxacc_svm.get_params(deep=True)['svc__gamma'], '): ')\n",
    "# class_acc(maxacc_svm.predict_proba(inp), outp, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Shadow Model\n",
    "Shadow model mimics the target network, emulating the target model's differences in prediction probabilities for samples in and out of its dataset. For this attack, only one shadow model is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "#Initialize models\n",
    "\n",
    "n_classes = valid_sequence_test_shadow.num_speakers\n",
    "print(n_classes)\n",
    "\n",
    "#NN\n",
    "shadow_net = shadow_net_type(in_size,n_hidden,n_classes).to(device)\n",
    "shadow_net.apply(models.weights_init)\n",
    "\n",
    "shadow_loss = nn.CrossEntropyLoss()\n",
    "shadow_optim = optim.Adam(shadow_net.parameters(), lr=lr)\n",
    "\n",
    "#SVM\n",
    "# shadowinputs, shadowtargets=load(sv_shadow_train_loader)\n",
    "# shadow_svm=make_pipeline(PCA(n_components=180), MinMaxScaler(feature_range=(-1,1)), \n",
    "#                          svm.SVC(C=1, gamma=.1, probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(shadowinputs))\n",
    "# shadowtargets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/2]\n",
      "Training:\n",
      "\n",
      "Accuracy = 70.94 %\n",
      "\n",
      "\n",
      "Test:\n",
      "\n",
      "Accuracy = 41.57 %\n",
      "\n",
      "\n",
      "[1/2]\n",
      "Training:\n",
      "\n",
      "Accuracy = 90.73 %\n",
      "\n",
      "\n",
      "Test:\n",
      "\n",
      "Accuracy = 58.98 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train NN\n",
    "\n",
    "# below commented code is for comparison during debugging\n",
    "# target_net = target_net_type(in_size,n_hidden,n_classes).to(device)\n",
    "# target_net.apply(models.weights_init)\n",
    "\n",
    "# target_loss = nn.CrossEntropyLoss()\n",
    "# target_optim = optim.Adam(target_net.parameters(), lr=lr)\n",
    "\n",
    "# train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs, verbose = False) #classes = range(valid_sequence_test.num_speakers),\n",
    "\n",
    "\n",
    "train(shadow_net, shadow_train_loader, shadow_test_loader, shadow_optim, shadow_loss, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train SVM\n",
    "# shadow_svm.fit(shadowinputs, shadowtargets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Attack Model\n",
    "A binary classifier to determine membership. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates two attack nets for comparison.\n",
    "\n",
    "# attack_net_svm = models.mlleaks_mlp(n_in=k).to(device)\n",
    "# attack_net_svm.apply(models.weights_init)\n",
    "\n",
    "attack_net_nn = models.mlleaks_mlp(n_in=k).to(device)\n",
    "attack_net_nn.apply(models.weights_init)\n",
    "\n",
    "attack_loss = nn.BCEWithLogitsLoss() #this one works\n",
    "# attack_loss = nn.BCELoss() # this one doesn't work \n",
    "# attack_optim_svm= optim.Adam(attack_net_svm.parameters(), lr=lr)\n",
    "attack_optim_nn= optim.Adam(attack_net_nn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains SVM attack model\n",
    "# train_attacker(attack_net_svm, shadow_svm, shadow_train_loader, shadow_out_loader, attack_optim_svm, attack_loss, n_epochs=2, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][0/174] loss = 0.74, accuracy = 50.00\n",
      "[0/50][1/174] loss = 0.73, accuracy = 50.00\n",
      "[0/50][2/174] loss = 0.73, accuracy = 50.00\n",
      "[0/50][3/174] loss = 0.73, accuracy = 50.00\n",
      "[0/50][4/174] loss = 0.72, accuracy = 50.00\n",
      "[0/50][5/174] loss = 0.72, accuracy = 50.00\n",
      "[0/50][6/174] loss = 0.72, accuracy = 50.00\n",
      "[0/50][7/174] loss = 0.71, accuracy = 50.00\n",
      "[0/50][8/174] loss = 0.71, accuracy = 50.00\n",
      "[0/50][9/174] loss = 0.71, accuracy = 50.00\n",
      "[0/50][10/174] loss = 0.70, accuracy = 50.00\n",
      "[0/50][11/174] loss = 0.70, accuracy = 50.00\n",
      "[0/50][12/174] loss = 0.70, accuracy = 50.00\n",
      "[0/50][13/174] loss = 0.70, accuracy = 50.00\n",
      "[0/50][14/174] loss = 0.70, accuracy = 50.00\n",
      "[0/50][15/174] loss = 0.70, accuracy = 50.00\n",
      "[0/50][16/174] loss = 0.70, accuracy = 50.00\n",
      "[0/50][17/174] loss = 0.69, accuracy = 49.83\n",
      "[0/50][18/174] loss = 0.70, accuracy = 48.85\n",
      "[0/50][19/174] loss = 0.70, accuracy = 47.58\n",
      "[0/50][20/174] loss = 0.70, accuracy = 47.17\n",
      "[0/50][21/174] loss = 0.70, accuracy = 46.80\n",
      "[0/50][22/174] loss = 0.70, accuracy = 46.94\n",
      "[0/50][23/174] loss = 0.70, accuracy = 47.14\n",
      "[0/50][24/174] loss = 0.70, accuracy = 47.25\n",
      "[0/50][25/174] loss = 0.70, accuracy = 47.36\n",
      "[0/50][26/174] loss = 0.70, accuracy = 47.45\n",
      "[0/50][27/174] loss = 0.70, accuracy = 47.54\n",
      "[0/50][28/174] loss = 0.70, accuracy = 47.63\n",
      "[0/50][29/174] loss = 0.70, accuracy = 47.71\n",
      "[0/50][30/174] loss = 0.70, accuracy = 47.78\n",
      "[0/50][31/174] loss = 0.70, accuracy = 47.85\n",
      "[0/50][32/174] loss = 0.70, accuracy = 47.92\n",
      "[0/50][33/174] loss = 0.69, accuracy = 47.98\n",
      "[0/50][34/174] loss = 0.70, accuracy = 48.04\n",
      "[0/50][35/174] loss = 0.70, accuracy = 48.09\n",
      "[0/50][36/174] loss = 0.70, accuracy = 48.10\n",
      "[0/50][37/174] loss = 0.70, accuracy = 48.11\n",
      "[0/50][38/174] loss = 0.70, accuracy = 48.16\n",
      "[0/50][39/174] loss = 0.70, accuracy = 47.93\n",
      "[0/50][40/174] loss = 0.70, accuracy = 47.64\n",
      "[0/50][41/174] loss = 0.69, accuracy = 47.58\n",
      "[0/50][42/174] loss = 0.70, accuracy = 47.38\n",
      "[0/50][43/174] loss = 0.69, accuracy = 47.30\n",
      "[0/50][44/174] loss = 0.69, accuracy = 47.15\n",
      "[0/50][45/174] loss = 0.69, accuracy = 47.15\n",
      "[0/50][46/174] loss = 0.70, accuracy = 47.21\n",
      "[0/50][47/174] loss = 0.70, accuracy = 47.27\n",
      "[0/50][48/174] loss = 0.69, accuracy = 47.32\n",
      "[0/50][49/174] loss = 0.69, accuracy = 47.38\n",
      "[0/50][50/174] loss = 0.69, accuracy = 47.43\n",
      "[0/50][51/174] loss = 0.70, accuracy = 47.48\n",
      "[0/50][52/174] loss = 0.70, accuracy = 47.52\n",
      "[0/50][53/174] loss = 0.69, accuracy = 47.57\n",
      "[0/50][54/174] loss = 0.69, accuracy = 47.61\n",
      "[0/50][55/174] loss = 0.69, accuracy = 47.66\n",
      "[0/50][56/174] loss = 0.69, accuracy = 47.70\n",
      "[0/50][57/174] loss = 0.69, accuracy = 47.74\n",
      "[0/50][58/174] loss = 0.69, accuracy = 47.78\n",
      "[0/50][59/174] loss = 0.69, accuracy = 47.81\n",
      "[0/50][60/174] loss = 0.69, accuracy = 47.85\n",
      "[0/50][61/174] loss = 0.69, accuracy = 47.88\n",
      "[0/50][62/174] loss = 0.69, accuracy = 47.92\n",
      "[0/50][63/174] loss = 0.69, accuracy = 47.95\n",
      "[0/50][64/174] loss = 0.69, accuracy = 47.91\n",
      "[0/50][65/174] loss = 0.69, accuracy = 47.94\n",
      "[0/50][66/174] loss = 0.69, accuracy = 47.99\n",
      "[0/50][67/174] loss = 0.69, accuracy = 48.00\n",
      "[0/50][68/174] loss = 0.69, accuracy = 48.10\n",
      "[0/50][69/174] loss = 0.69, accuracy = 48.04\n",
      "[0/50][70/174] loss = 0.69, accuracy = 48.17\n",
      "[0/50][71/174] loss = 0.69, accuracy = 48.26\n",
      "[0/50][72/174] loss = 0.69, accuracy = 48.37\n",
      "[0/50][73/174] loss = 0.69, accuracy = 48.52\n",
      "[0/50][74/174] loss = 0.69, accuracy = 48.75\n",
      "[0/50][75/174] loss = 0.69, accuracy = 48.81\n",
      "[0/50][76/174] loss = 0.69, accuracy = 48.90\n",
      "[0/50][77/174] loss = 0.69, accuracy = 49.04\n",
      "[0/50][78/174] loss = 0.69, accuracy = 49.23\n",
      "[0/50][79/174] loss = 0.69, accuracy = 49.34\n",
      "[0/50][80/174] loss = 0.69, accuracy = 49.40\n",
      "[0/50][81/174] loss = 0.69, accuracy = 49.54\n",
      "[0/50][82/174] loss = 0.69, accuracy = 49.62\n",
      "[0/50][83/174] loss = 0.69, accuracy = 49.83\n",
      "[0/50][84/174] loss = 0.69, accuracy = 50.04\n",
      "[0/50][85/174] loss = 0.69, accuracy = 50.20\n",
      "[0/50][86/174] loss = 0.69, accuracy = 50.36\n",
      "[0/50][87/174] loss = 0.69, accuracy = 50.43\n",
      "[0/50][88/174] loss = 0.69, accuracy = 50.56\n",
      "[0/50][89/174] loss = 0.69, accuracy = 50.57\n",
      "[0/50][90/174] loss = 0.69, accuracy = 50.69\n",
      "[0/50][91/174] loss = 0.69, accuracy = 50.76\n",
      "[0/50][92/174] loss = 0.69, accuracy = 50.82\n",
      "[0/50][93/174] loss = 0.69, accuracy = 50.93\n",
      "[0/50][94/174] loss = 0.69, accuracy = 51.04\n",
      "[0/50][95/174] loss = 0.69, accuracy = 51.11\n",
      "[0/50][96/174] loss = 0.69, accuracy = 51.13\n",
      "[0/50][97/174] loss = 0.69, accuracy = 51.23\n",
      "[0/50][98/174] loss = 0.69, accuracy = 51.36\n",
      "[0/50][99/174] loss = 0.69, accuracy = 51.48\n",
      "[0/50][100/174] loss = 0.69, accuracy = 51.66\n",
      "[0/50][101/174] loss = 0.69, accuracy = 51.79\n",
      "[0/50][102/174] loss = 0.69, accuracy = 51.85\n",
      "[0/50][103/174] loss = 0.69, accuracy = 51.91\n",
      "[0/50][104/174] loss = 0.69, accuracy = 52.05\n",
      "[0/50][105/174] loss = 0.69, accuracy = 52.14\n",
      "[0/50][106/174] loss = 0.69, accuracy = 52.25\n",
      "[0/50][107/174] loss = 0.69, accuracy = 52.36\n",
      "[0/50][108/174] loss = 0.69, accuracy = 52.47\n",
      "[0/50][109/174] loss = 0.69, accuracy = 52.54\n",
      "[0/50][110/174] loss = 0.69, accuracy = 52.60\n",
      "[0/50][111/174] loss = 0.69, accuracy = 52.62\n",
      "[0/50][112/174] loss = 0.69, accuracy = 52.72\n",
      "[0/50][113/174] loss = 0.69, accuracy = 52.82\n",
      "[0/50][114/174] loss = 0.69, accuracy = 52.95\n",
      "[0/50][115/174] loss = 0.69, accuracy = 52.99\n",
      "[0/50][116/174] loss = 0.69, accuracy = 53.10\n",
      "[0/50][117/174] loss = 0.69, accuracy = 53.11\n",
      "[0/50][118/174] loss = 0.69, accuracy = 53.20\n",
      "[0/50][119/174] loss = 0.69, accuracy = 53.31\n",
      "[0/50][120/174] loss = 0.69, accuracy = 53.31\n",
      "[0/50][121/174] loss = 0.69, accuracy = 53.37\n",
      "[0/50][122/174] loss = 0.69, accuracy = 53.33\n",
      "[0/50][123/174] loss = 0.68, accuracy = 53.49\n",
      "[0/50][124/174] loss = 0.69, accuracy = 53.55\n",
      "[0/50][125/174] loss = 0.69, accuracy = 53.58\n",
      "[0/50][126/174] loss = 0.69, accuracy = 53.64\n",
      "[0/50][127/174] loss = 0.69, accuracy = 53.72\n",
      "[0/50][128/174] loss = 0.69, accuracy = 53.65\n",
      "[0/50][129/174] loss = 0.68, accuracy = 53.75\n",
      "[0/50][130/174] loss = 0.69, accuracy = 53.80\n",
      "[0/50][131/174] loss = 0.69, accuracy = 53.85\n",
      "[0/50][132/174] loss = 0.69, accuracy = 53.94\n",
      "[0/50][133/174] loss = 0.69, accuracy = 53.96\n",
      "[0/50][134/174] loss = 0.69, accuracy = 53.98\n",
      "[0/50][135/174] loss = 0.68, accuracy = 54.06\n",
      "[0/50][136/174] loss = 0.69, accuracy = 54.11\n",
      "[0/50][137/174] loss = 0.68, accuracy = 54.27\n",
      "[0/50][138/174] loss = 0.69, accuracy = 54.29\n",
      "[0/50][139/174] loss = 0.69, accuracy = 54.33\n",
      "[0/50][140/174] loss = 0.69, accuracy = 54.42\n",
      "[0/50][141/174] loss = 0.69, accuracy = 54.46\n",
      "[0/50][142/174] loss = 0.68, accuracy = 54.53\n",
      "[0/50][143/174] loss = 0.69, accuracy = 54.57\n",
      "[0/50][144/174] loss = 0.69, accuracy = 54.60\n",
      "[0/50][145/174] loss = 0.68, accuracy = 54.77\n",
      "[0/50][146/174] loss = 0.69, accuracy = 54.83\n",
      "[0/50][147/174] loss = 0.69, accuracy = 54.90\n",
      "[0/50][148/174] loss = 0.69, accuracy = 54.89\n",
      "[0/50][149/174] loss = 0.68, accuracy = 54.97\n",
      "[0/50][150/174] loss = 0.69, accuracy = 55.04\n",
      "[0/50][151/174] loss = 0.68, accuracy = 55.14\n",
      "[0/50][152/174] loss = 0.68, accuracy = 55.23\n",
      "[0/50][153/174] loss = 0.68, accuracy = 55.29\n",
      "[0/50][154/174] loss = 0.69, accuracy = 55.29\n",
      "[0/50][155/174] loss = 0.68, accuracy = 55.39\n",
      "[0/50][156/174] loss = 0.68, accuracy = 55.50\n",
      "[0/50][157/174] loss = 0.68, accuracy = 55.60\n",
      "[0/50][158/174] loss = 0.68, accuracy = 55.64\n",
      "[0/50][159/174] loss = 0.69, accuracy = 55.63\n",
      "[0/50][160/174] loss = 0.69, accuracy = 55.61\n",
      "[0/50][161/174] loss = 0.68, accuracy = 55.70\n",
      "[0/50][162/174] loss = 0.69, accuracy = 55.71\n",
      "[0/50][163/174] loss = 0.69, accuracy = 55.72\n",
      "[0/50][164/174] loss = 0.68, accuracy = 55.81\n",
      "[0/50][165/174] loss = 0.68, accuracy = 55.88\n",
      "[0/50][166/174] loss = 0.68, accuracy = 55.90\n",
      "[0/50][167/174] loss = 0.68, accuracy = 56.01\n",
      "[0/50][168/174] loss = 0.69, accuracy = 56.04\n",
      "[0/50][169/174] loss = 0.68, accuracy = 56.06\n",
      "[0/50][170/174] loss = 0.68, accuracy = 56.14\n",
      "[0/50][171/174] loss = 0.68, accuracy = 56.21\n",
      "[0/50][172/174] loss = 0.68, accuracy = 56.27\n",
      "[0/50][173/174] loss = 0.67, accuracy = 56.31\n",
      "[1/50][0/174] loss = 0.68, accuracy = 65.62\n",
      "[1/50][1/174] loss = 0.68, accuracy = 60.94\n",
      "[1/50][2/174] loss = 0.69, accuracy = 60.42\n",
      "[1/50][3/174] loss = 0.68, accuracy = 62.11\n",
      "[1/50][4/174] loss = 0.68, accuracy = 62.19\n",
      "[1/50][5/174] loss = 0.68, accuracy = 62.76\n",
      "[1/50][6/174] loss = 0.68, accuracy = 62.95\n",
      "[1/50][7/174] loss = 0.69, accuracy = 62.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][8/174] loss = 0.68, accuracy = 62.85\n",
      "[1/50][9/174] loss = 0.68, accuracy = 62.66\n",
      "[1/50][10/174] loss = 0.68, accuracy = 62.93\n",
      "[1/50][11/174] loss = 0.68, accuracy = 62.89\n",
      "[1/50][12/174] loss = 0.68, accuracy = 62.86\n",
      "[1/50][13/174] loss = 0.69, accuracy = 62.39\n",
      "[1/50][14/174] loss = 0.67, accuracy = 62.81\n",
      "[1/50][15/174] loss = 0.67, accuracy = 63.28\n",
      "[1/50][16/174] loss = 0.68, accuracy = 63.42\n",
      "[1/50][17/174] loss = 0.67, accuracy = 63.37\n",
      "[1/50][18/174] loss = 0.68, accuracy = 63.24\n",
      "[1/50][19/174] loss = 0.67, accuracy = 63.59\n",
      "[1/50][20/174] loss = 0.67, accuracy = 63.69\n",
      "[1/50][21/174] loss = 0.68, accuracy = 63.64\n",
      "[1/50][22/174] loss = 0.67, accuracy = 63.99\n",
      "[1/50][23/174] loss = 0.69, accuracy = 63.67\n",
      "[1/50][24/174] loss = 0.69, accuracy = 63.44\n",
      "[1/50][25/174] loss = 0.68, accuracy = 63.58\n",
      "[1/50][26/174] loss = 0.67, accuracy = 63.66\n",
      "[1/50][27/174] loss = 0.70, accuracy = 62.95\n",
      "[1/50][28/174] loss = 0.68, accuracy = 62.77\n",
      "[1/50][29/174] loss = 0.69, accuracy = 62.66\n",
      "[1/50][30/174] loss = 0.68, accuracy = 62.70\n",
      "[1/50][31/174] loss = 0.68, accuracy = 62.60\n",
      "[1/50][32/174] loss = 0.68, accuracy = 62.64\n",
      "[1/50][33/174] loss = 0.68, accuracy = 62.78\n",
      "[1/50][34/174] loss = 0.68, accuracy = 62.99\n",
      "[1/50][35/174] loss = 0.69, accuracy = 62.63\n",
      "[1/50][36/174] loss = 0.67, accuracy = 62.75\n",
      "[1/50][37/174] loss = 0.67, accuracy = 62.87\n",
      "[1/50][38/174] loss = 0.68, accuracy = 62.98\n",
      "[1/50][39/174] loss = 0.67, accuracy = 63.12\n",
      "[1/50][40/174] loss = 0.68, accuracy = 63.00\n",
      "[1/50][41/174] loss = 0.67, accuracy = 63.13\n",
      "[1/50][42/174] loss = 0.68, accuracy = 63.05\n",
      "[1/50][43/174] loss = 0.68, accuracy = 63.07\n",
      "[1/50][44/174] loss = 0.67, accuracy = 63.16\n",
      "[1/50][45/174] loss = 0.68, accuracy = 63.08\n",
      "[1/50][46/174] loss = 0.68, accuracy = 63.03\n",
      "[1/50][47/174] loss = 0.68, accuracy = 62.83\n",
      "[1/50][48/174] loss = 0.68, accuracy = 62.79\n",
      "[1/50][49/174] loss = 0.68, accuracy = 62.69\n",
      "[1/50][50/174] loss = 0.68, accuracy = 62.59\n",
      "[1/50][51/174] loss = 0.69, accuracy = 62.47\n",
      "[1/50][52/174] loss = 0.67, accuracy = 62.50\n",
      "[1/50][53/174] loss = 0.69, accuracy = 62.36\n",
      "[1/50][54/174] loss = 0.67, accuracy = 62.47\n",
      "[1/50][55/174] loss = 0.68, accuracy = 62.39\n",
      "[1/50][56/174] loss = 0.67, accuracy = 62.42\n",
      "[1/50][57/174] loss = 0.68, accuracy = 62.37\n",
      "[1/50][58/174] loss = 0.67, accuracy = 62.50\n",
      "[1/50][59/174] loss = 0.69, accuracy = 62.37\n",
      "[1/50][60/174] loss = 0.67, accuracy = 62.55\n",
      "[1/50][61/174] loss = 0.69, accuracy = 62.50\n",
      "[1/50][62/174] loss = 0.67, accuracy = 62.60\n",
      "[1/50][63/174] loss = 0.66, accuracy = 62.79\n",
      "[1/50][64/174] loss = 0.68, accuracy = 62.84\n",
      "[1/50][65/174] loss = 0.67, accuracy = 62.88\n",
      "[1/50][66/174] loss = 0.66, accuracy = 63.08\n",
      "[1/50][67/174] loss = 0.68, accuracy = 63.03\n",
      "[1/50][68/174] loss = 0.68, accuracy = 62.93\n",
      "[1/50][69/174] loss = 0.68, accuracy = 62.86\n",
      "[1/50][70/174] loss = 0.67, accuracy = 62.98\n",
      "[1/50][71/174] loss = 0.68, accuracy = 62.98\n",
      "[1/50][72/174] loss = 0.69, accuracy = 62.82\n",
      "[1/50][73/174] loss = 0.68, accuracy = 62.75\n",
      "[1/50][74/174] loss = 0.67, accuracy = 62.83\n",
      "[1/50][75/174] loss = 0.67, accuracy = 62.85\n",
      "[1/50][76/174] loss = 0.66, accuracy = 62.99\n",
      "[1/50][77/174] loss = 0.68, accuracy = 62.90\n",
      "[1/50][78/174] loss = 0.69, accuracy = 62.84\n",
      "[1/50][79/174] loss = 0.67, accuracy = 62.91\n",
      "[1/50][80/174] loss = 0.68, accuracy = 62.91\n",
      "[1/50][81/174] loss = 0.67, accuracy = 63.00\n",
      "[1/50][82/174] loss = 0.67, accuracy = 63.03\n",
      "[1/50][83/174] loss = 0.67, accuracy = 63.04\n",
      "[1/50][84/174] loss = 0.68, accuracy = 63.05\n",
      "[1/50][85/174] loss = 0.68, accuracy = 62.95\n",
      "[1/50][86/174] loss = 0.67, accuracy = 63.00\n",
      "[1/50][87/174] loss = 0.66, accuracy = 63.07\n",
      "[1/50][88/174] loss = 0.67, accuracy = 63.04\n",
      "[1/50][89/174] loss = 0.66, accuracy = 63.14\n",
      "[1/50][90/174] loss = 0.67, accuracy = 63.19\n",
      "[1/50][91/174] loss = 0.69, accuracy = 63.06\n",
      "[1/50][92/174] loss = 0.67, accuracy = 63.07\n",
      "[1/50][93/174] loss = 0.66, accuracy = 63.12\n",
      "[1/50][94/174] loss = 0.68, accuracy = 63.08\n",
      "[1/50][95/174] loss = 0.65, accuracy = 63.22\n",
      "[1/50][96/174] loss = 0.68, accuracy = 63.16\n",
      "[1/50][97/174] loss = 0.67, accuracy = 63.15\n",
      "[1/50][98/174] loss = 0.68, accuracy = 63.13\n",
      "[1/50][99/174] loss = 0.70, accuracy = 63.03\n",
      "[1/50][100/174] loss = 0.68, accuracy = 62.98\n",
      "[1/50][101/174] loss = 0.65, accuracy = 63.08\n",
      "[1/50][102/174] loss = 0.67, accuracy = 63.14\n",
      "[1/50][103/174] loss = 0.66, accuracy = 63.22\n",
      "[1/50][104/174] loss = 0.70, accuracy = 63.08\n",
      "[1/50][105/174] loss = 0.69, accuracy = 63.02\n",
      "[1/50][106/174] loss = 0.68, accuracy = 62.95\n",
      "[1/50][107/174] loss = 0.65, accuracy = 63.02\n",
      "[1/50][108/174] loss = 0.67, accuracy = 63.00\n",
      "[1/50][109/174] loss = 0.67, accuracy = 62.98\n",
      "[1/50][110/174] loss = 0.68, accuracy = 62.95\n",
      "[1/50][111/174] loss = 0.67, accuracy = 62.93\n",
      "[1/50][112/174] loss = 0.69, accuracy = 62.82\n",
      "[1/50][113/174] loss = 0.69, accuracy = 62.75\n",
      "[1/50][114/174] loss = 0.68, accuracy = 62.72\n",
      "[1/50][115/174] loss = 0.67, accuracy = 62.76\n",
      "[1/50][116/174] loss = 0.67, accuracy = 62.85\n",
      "[1/50][117/174] loss = 0.67, accuracy = 62.84\n",
      "[1/50][118/174] loss = 0.69, accuracy = 62.80\n",
      "[1/50][119/174] loss = 0.67, accuracy = 62.80\n",
      "[1/50][120/174] loss = 0.69, accuracy = 62.77\n",
      "[1/50][121/174] loss = 0.67, accuracy = 62.79\n",
      "[1/50][122/174] loss = 0.67, accuracy = 62.77\n",
      "[1/50][123/174] loss = 0.68, accuracy = 62.76\n",
      "[1/50][124/174] loss = 0.67, accuracy = 62.69\n",
      "[1/50][125/174] loss = 0.67, accuracy = 62.70\n",
      "[1/50][126/174] loss = 0.66, accuracy = 62.78\n",
      "[1/50][127/174] loss = 0.66, accuracy = 62.85\n",
      "[1/50][128/174] loss = 0.70, accuracy = 62.74\n",
      "[1/50][129/174] loss = 0.66, accuracy = 62.81\n",
      "[1/50][130/174] loss = 0.67, accuracy = 62.77\n",
      "[1/50][131/174] loss = 0.67, accuracy = 62.75\n",
      "[1/50][132/174] loss = 0.65, accuracy = 62.84\n",
      "[1/50][133/174] loss = 0.66, accuracy = 62.86\n",
      "[1/50][134/174] loss = 0.67, accuracy = 62.80\n",
      "[1/50][135/174] loss = 0.65, accuracy = 62.83\n",
      "[1/50][136/174] loss = 0.67, accuracy = 62.82\n",
      "[1/50][137/174] loss = 0.67, accuracy = 62.81\n",
      "[1/50][138/174] loss = 0.68, accuracy = 62.75\n",
      "[1/50][139/174] loss = 0.65, accuracy = 62.80\n",
      "[1/50][140/174] loss = 0.65, accuracy = 62.84\n",
      "[1/50][141/174] loss = 0.66, accuracy = 62.89\n",
      "[1/50][142/174] loss = 0.67, accuracy = 62.84\n",
      "[1/50][143/174] loss = 0.67, accuracy = 62.85\n",
      "[1/50][144/174] loss = 0.67, accuracy = 62.84\n",
      "[1/50][145/174] loss = 0.67, accuracy = 62.84\n",
      "[1/50][146/174] loss = 0.66, accuracy = 62.82\n",
      "[1/50][147/174] loss = 0.67, accuracy = 62.81\n",
      "[1/50][148/174] loss = 0.67, accuracy = 62.81\n",
      "[1/50][149/174] loss = 0.66, accuracy = 62.85\n",
      "[1/50][150/174] loss = 0.66, accuracy = 62.89\n",
      "[1/50][151/174] loss = 0.67, accuracy = 62.91\n",
      "[1/50][152/174] loss = 0.66, accuracy = 62.94\n",
      "[1/50][153/174] loss = 0.68, accuracy = 62.89\n",
      "[1/50][154/174] loss = 0.69, accuracy = 62.85\n",
      "[1/50][155/174] loss = 0.66, accuracy = 62.87\n",
      "[1/50][156/174] loss = 0.66, accuracy = 62.90\n",
      "[1/50][157/174] loss = 0.69, accuracy = 62.85\n",
      "[1/50][158/174] loss = 0.67, accuracy = 62.83\n",
      "[1/50][159/174] loss = 0.66, accuracy = 62.84\n",
      "[1/50][160/174] loss = 0.68, accuracy = 62.77\n",
      "[1/50][161/174] loss = 0.65, accuracy = 62.83\n",
      "[1/50][162/174] loss = 0.67, accuracy = 62.84\n",
      "[1/50][163/174] loss = 0.67, accuracy = 62.83\n",
      "[1/50][164/174] loss = 0.68, accuracy = 62.81\n",
      "[1/50][165/174] loss = 0.65, accuracy = 62.83\n",
      "[1/50][166/174] loss = 0.67, accuracy = 62.82\n",
      "[1/50][167/174] loss = 0.68, accuracy = 62.79\n",
      "[1/50][168/174] loss = 0.66, accuracy = 62.80\n",
      "[1/50][169/174] loss = 0.68, accuracy = 62.77\n",
      "[1/50][170/174] loss = 0.66, accuracy = 62.80\n",
      "[1/50][171/174] loss = 0.67, accuracy = 62.79\n",
      "[1/50][172/174] loss = 0.67, accuracy = 62.78\n",
      "[1/50][173/174] loss = 0.60, accuracy = 62.81\n",
      "[2/50][0/174] loss = 0.66, accuracy = 68.75\n",
      "[2/50][1/174] loss = 0.67, accuracy = 63.28\n",
      "[2/50][2/174] loss = 0.66, accuracy = 63.02\n",
      "[2/50][3/174] loss = 0.65, accuracy = 64.45\n",
      "[2/50][4/174] loss = 0.67, accuracy = 63.44\n",
      "[2/50][5/174] loss = 0.69, accuracy = 61.20\n",
      "[2/50][6/174] loss = 0.66, accuracy = 61.16\n",
      "[2/50][7/174] loss = 0.66, accuracy = 61.72\n",
      "[2/50][8/174] loss = 0.68, accuracy = 61.46\n",
      "[2/50][9/174] loss = 0.67, accuracy = 62.19\n",
      "[2/50][10/174] loss = 0.66, accuracy = 62.36\n",
      "[2/50][11/174] loss = 0.64, accuracy = 62.76\n",
      "[2/50][12/174] loss = 0.68, accuracy = 62.50\n",
      "[2/50][13/174] loss = 0.67, accuracy = 62.72\n",
      "[2/50][14/174] loss = 0.66, accuracy = 62.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/50][15/174] loss = 0.65, accuracy = 62.70\n",
      "[2/50][16/174] loss = 0.66, accuracy = 62.68\n",
      "[2/50][17/174] loss = 0.65, accuracy = 62.93\n",
      "[2/50][18/174] loss = 0.63, accuracy = 63.65\n",
      "[2/50][19/174] loss = 0.63, accuracy = 63.98\n",
      "[2/50][20/174] loss = 0.64, accuracy = 64.43\n",
      "[2/50][21/174] loss = 0.70, accuracy = 63.78\n",
      "[2/50][22/174] loss = 0.67, accuracy = 63.52\n",
      "[2/50][23/174] loss = 0.69, accuracy = 62.76\n",
      "[2/50][24/174] loss = 0.66, accuracy = 62.94\n",
      "[2/50][25/174] loss = 0.66, accuracy = 62.98\n",
      "[2/50][26/174] loss = 0.64, accuracy = 63.25\n",
      "[2/50][27/174] loss = 0.66, accuracy = 63.39\n",
      "[2/50][28/174] loss = 0.67, accuracy = 63.15\n",
      "[2/50][29/174] loss = 0.66, accuracy = 63.12\n",
      "[2/50][30/174] loss = 0.68, accuracy = 63.00\n",
      "[2/50][31/174] loss = 0.66, accuracy = 63.09\n",
      "[2/50][32/174] loss = 0.68, accuracy = 63.07\n",
      "[2/50][33/174] loss = 0.65, accuracy = 63.19\n",
      "[2/50][34/174] loss = 0.64, accuracy = 63.35\n",
      "[2/50][35/174] loss = 0.66, accuracy = 63.45\n",
      "[2/50][36/174] loss = 0.65, accuracy = 63.51\n",
      "[2/50][37/174] loss = 0.64, accuracy = 63.69\n",
      "[2/50][38/174] loss = 0.67, accuracy = 63.54\n",
      "[2/50][39/174] loss = 0.67, accuracy = 63.36\n",
      "[2/50][40/174] loss = 0.67, accuracy = 63.26\n",
      "[2/50][41/174] loss = 0.67, accuracy = 63.24\n",
      "[2/50][42/174] loss = 0.63, accuracy = 63.44\n",
      "[2/50][43/174] loss = 0.67, accuracy = 63.39\n",
      "[2/50][44/174] loss = 0.67, accuracy = 63.26\n",
      "[2/50][45/174] loss = 0.63, accuracy = 63.45\n",
      "[2/50][46/174] loss = 0.63, accuracy = 63.73\n",
      "[2/50][47/174] loss = 0.65, accuracy = 63.74\n",
      "[2/50][48/174] loss = 0.70, accuracy = 63.49\n",
      "[2/50][49/174] loss = 0.66, accuracy = 63.50\n",
      "[2/50][50/174] loss = 0.68, accuracy = 63.39\n",
      "[2/50][51/174] loss = 0.67, accuracy = 63.31\n",
      "[2/50][52/174] loss = 0.67, accuracy = 63.27\n",
      "[2/50][53/174] loss = 0.64, accuracy = 63.45\n",
      "[2/50][54/174] loss = 0.66, accuracy = 63.38\n",
      "[2/50][55/174] loss = 0.68, accuracy = 63.25\n",
      "[2/50][56/174] loss = 0.64, accuracy = 63.35\n",
      "[2/50][57/174] loss = 0.69, accuracy = 63.20\n",
      "[2/50][58/174] loss = 0.65, accuracy = 63.19\n",
      "[2/50][59/174] loss = 0.65, accuracy = 63.26\n",
      "[2/50][60/174] loss = 0.67, accuracy = 63.19\n",
      "[2/50][61/174] loss = 0.67, accuracy = 63.16\n",
      "[2/50][62/174] loss = 0.66, accuracy = 63.19\n",
      "[2/50][63/174] loss = 0.66, accuracy = 63.21\n",
      "[2/50][64/174] loss = 0.68, accuracy = 63.12\n",
      "[2/50][65/174] loss = 0.65, accuracy = 63.19\n",
      "[2/50][66/174] loss = 0.65, accuracy = 63.25\n",
      "[2/50][67/174] loss = 0.68, accuracy = 63.12\n",
      "[2/50][68/174] loss = 0.65, accuracy = 63.22\n",
      "[2/50][69/174] loss = 0.67, accuracy = 63.15\n",
      "[2/50][70/174] loss = 0.62, accuracy = 63.29\n",
      "[2/50][71/174] loss = 0.65, accuracy = 63.37\n",
      "[2/50][72/174] loss = 0.66, accuracy = 63.31\n",
      "[2/50][73/174] loss = 0.65, accuracy = 63.37\n",
      "[2/50][74/174] loss = 0.67, accuracy = 63.35\n",
      "[2/50][75/174] loss = 0.66, accuracy = 63.30\n",
      "[2/50][76/174] loss = 0.63, accuracy = 63.33\n",
      "[2/50][77/174] loss = 0.67, accuracy = 63.32\n",
      "[2/50][78/174] loss = 0.66, accuracy = 63.33\n",
      "[2/50][79/174] loss = 0.67, accuracy = 63.30\n",
      "[2/50][80/174] loss = 0.66, accuracy = 63.35\n",
      "[2/50][81/174] loss = 0.67, accuracy = 63.32\n",
      "[2/50][82/174] loss = 0.67, accuracy = 63.31\n",
      "[2/50][83/174] loss = 0.64, accuracy = 63.37\n",
      "[2/50][84/174] loss = 0.66, accuracy = 63.33\n",
      "[2/50][85/174] loss = 0.64, accuracy = 63.43\n",
      "[2/50][86/174] loss = 0.67, accuracy = 63.42\n",
      "[2/50][87/174] loss = 0.66, accuracy = 63.41\n"
     ]
    }
   ],
   "source": [
    "#Trains NN attack model\n",
    "\n",
    "train_attacker(attack_net_nn, shadow_net, shadow_train_loader, shadow_out_loader, attack_optim_nn, attack_loss, n_epochs=50, k=k)\n",
    "\n",
    "#original:\n",
    "# train_attacker(attack_net_nn, shadow_net, shadow_train_loader, shadow_out_loader, attack_optim_nn, attack_loss, n_epochs=50, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Attack Nets\n",
    "How well the trained attack models classify a sample as in or out of a target model's training dataset, and how performance is affected by target hyperparameters and which models attack which targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#attack net trained on svm shadow model on svm target, C=10, gamma=.1\n",
    "# eval_attack_net(attack_net_svm, gen_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack net trained on svm shadow model on svm target, C=1, gamma=.01\n",
    "# eval_attack_net(attack_net_svm, maxacc_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#attack net trained on nn shadow model on nn target\n",
    "eval_attack_net(attack_net_nn, target_net, target_train_loader, target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack net trained on nn shadow model on svm target, C=10, gamma=.1\n",
    "# eval_attack_net(attack_net_nn, gen_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
