{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.5 (default, Jul  6 2018, 19:12:46) \n",
      "[GCC 5.4.0 20160609]\n",
      "Pytorch: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.insert(0, '../../../Utils/')\n",
    "\n",
    "import models\n",
    "from train import *\n",
    "from metrics import * \n",
    "from SVC_Utils import *\n",
    "\n",
    "#audio\n",
    "import librosa as libr\n",
    "\n",
    "print(\"Python: %s\" % sys.version)\n",
    "print(\"Pytorch: %s\" % torch.__version__)\n",
    "\n",
    "# determine device to run network on (runs on gpu if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.001\n",
    "k = 3\n",
    "\n",
    "pretrained = True #run this with networks that have already been trained\n",
    "\n",
    "transform_type = 'SFTF' #either STFT or MFCC  \n",
    "\n",
    "if transform_type == 'SFTF':\n",
    "    target_net_type = models.STFT_CNN_classifier\n",
    "    shadow_net_type = models.STFT_CNN_classifier\n",
    "    in_size = 94# 20 forMFCC,  94 for STFT\n",
    "elif transform_type == 'MFCC':\n",
    "    target_net_type = models.audio_CNN_classifier\n",
    "    shadow_net_type = models.audio_CNN_classifier\n",
    "    in_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speech preprocessing\n",
    "\n",
    "class tensorToMFCC:\n",
    "    def __call__(self, y):\n",
    "#         y = y.numpy()\n",
    "        dims = y.shape\n",
    "        y = libr.feature.melspectrogram(np.reshape(y, (dims[1],)), 16000, n_mels=number_of_mels,\n",
    "                               fmax=8000)\n",
    "        y = libr.feature.mfcc(S = libr.power_to_db(y))\n",
    "        y = torch.from_numpy(y)                           \n",
    "        return y.float()\n",
    "\n",
    "class STFT:\n",
    "    def __call__(self,y):\n",
    "        dims = y.shape\n",
    "        y = np.abs(libr.core.stft(np.reshape(y, (dims[1],))))\n",
    "        y = torch.from_numpy(y).permute(1,0)\n",
    "        return y.float()\n",
    "    \n",
    "if transform_type == 'SFTF':\n",
    "    transform  = STFT() ## STFT or MFCC\n",
    "elif transform_type == 'MFCC':\n",
    "    transform  = tensorToMFCC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seconds = 3\n",
    "n_epochs = 25\n",
    "sampling_rate = 16000\n",
    "number_of_mels =128\n",
    "lr = 0.001\n",
    "\n",
    "splits = [0.8, 0.2] #input fraction of data you want partitioned. Train, test fraction\n",
    "attacking = 1 #0 for no attack, 1 for attack 1, 3 for attack 3\n",
    "\n",
    "# attacking means data for a target & shadow network.\n",
    "# This will also split \"out data\" from totally different speakers -- data none of the \n",
    "# other networks have seen, for training & testing the attack network. This will be\n",
    "# an equivalent amount of data to the train split as defined about\n",
    "\n",
    "if sum(splits) != 1:\n",
    "    print('error: splits do not sum to 1.')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, './../../../Utils')\n",
    "from datasets import LibriSpeechDataset\n",
    "from datasets import audio_preload_and_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VOiCES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising VOiCES Dataset with minimum length = 3s and subsets = []\n",
      "['part_6', 'part_4', 'Lab41-SRI-VOiCES-speaker-gender-dataset_SUBSET.csv', 'part_2', 'part_8', 'part_1', 'part_5', 'part_3', 'part_7']\n",
      "   Speaker Gender          DataSet\n",
      "0     5126      M  train-clean-360\n",
      "1     3549      F  train-clean-360\n",
      "2     4331      F  train-clean-360\n",
      "3      196      M  train-clean-100\n",
      "4     2289      M  train-clean-100\n",
      "   speaker_id sex           subset\n",
      "0        5126   M  train-clean-360\n",
      "1        3549   F  train-clean-360\n",
      "2        4331   F  train-clean-360\n",
      "3         196   M  train-clean-100\n",
      "4        2289   M  train-clean-100\n",
      "Indexing part_6...\n",
      "Indexing part_4...\n",
      "Indexing part_2...\n",
      "Indexing part_8...\n",
      "Indexing part_1...\n",
      "Indexing part_5...\n",
      "Indexing part_3...\n",
      "Indexing part_7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "MergeError",
     "evalue": "No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b7f58d017262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Splits data into 2 sets of speakers for target & shadow network, into above defined train:test splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_preload_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattacking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattacking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'VOiCES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/fs4/home/nlopatina/cyphercat/Utils/datasets.py\u001b[0m in \u001b[0;36maudio_preload_and_split\u001b[0;34m(path, subsets, seconds, pad, cache, splits, attacking, data)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Merge individual audio files with indexing dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# # Concatenate with already existing dataframe if any exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     59\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                          validate=validate)\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_specification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# note this function has side effects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_validate_specification\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m                         \u001b[0;34m'left_index={lidx}, right_index={ridx}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                         .format(lon=self.left_on, ron=self.right_on,\n\u001b[0;32m-> 1035\u001b[0;31m                                 lidx=self.left_index, ridx=self.right_index))\n\u001b[0m\u001b[1;32m   1036\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcommon_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                     raise MergeError(\"Data columns not unique: {common!r}\"\n",
      "\u001b[0;31mMergeError\u001b[0m: No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False"
     ]
    }
   ],
   "source": [
    "# directory path for VOiCES  dataset\n",
    "path = os.path.join(os.getcwd(),'../../../Datasets/VOiCES/') #'add/yourDirectory/path/VOiCES'\n",
    "all_data = ['train-clean-360','train-clean-100','dev-clean']\n",
    "all_data = []\n",
    "\n",
    "#Splits data into 2 sets of speakers for target & shadow network, into above defined train:test splits\n",
    "dfs = audio_preload_and_split(path,all_data,n_seconds,pad=False,cache=True,splits=splits, attacking = attacking, data ='VOiCES')  \n",
    "\n",
    "\n",
    "#reference files\n",
    "spkGendr = pd.read_csv(path+'Lab41-SRI-VOiCES-speaker-gender-dataset_SUBSET.csv')\n",
    "\n",
    "print('Total number of speakers: ', len(spkGendr))\n",
    "print('Number of females:', spkGendr.Gender[spkGendr.Gender == 'F'].count())\n",
    "print('Number of males:', spkGendr.Gender[spkGendr.Gender == 'F'].count())\n",
    "\n",
    "dfs = audio_preload_and_split(path,all_data,n_seconds,pad=False,cache=True,splits=splits, attacking = attacking, data ='Libri')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LibriSpeech data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Librivox data set (55GB) into ./../../../Classification_baselines/LibriSpeech/data/ if not already present...\n",
      "Found archive \"./../../../Classification_baselines/LibriSpeech/data/train-clean-100.tar.gz\" - not downloading.\n",
      "Found archive \"./../../../Classification_baselines/LibriSpeech/data/train-clean-360.tar.gz\" - not downloading.\n",
      "No archive \"./../../../Classification_baselines/LibriSpeech/data/train-other-500.tar.gz\" - downloading...\n"
     ]
    }
   ],
   "source": [
    "### Libri hyperparameters\n",
    "\n",
    "data_set = 100 #100 or 360\n",
    "if data_set == 100:\n",
    "    all_data = ['train-clean-100']\n",
    "elif data_set == 360:\n",
    "    all_data = ['train-clean-360']\n",
    "\n",
    "### Data set\n",
    "path = './../../../Classification_baselines/LibriSpeech/data/'\n",
    "\n",
    "# #to load more data sets\n",
    "# import data_downloaders\n",
    "# data_downloaders._download_and_preprocess_data(path)\n",
    "\n",
    "#Splits data into 2 sets of speakers for target & shadow network, into above defined train:test splits\n",
    "dfs = audio_preload_and_split(path,all_data,n_seconds,pad=False,cache=True,splits=splits, attacking = attacking, data ='Libri')  \n",
    "\n",
    "#target train & test\n",
    "valid_sequence_train = LibriSpeechDataset(path, df = dfs[0], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_test = LibriSpeechDataset(path, df = dfs[1], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "#shadow train & test\n",
    "valid_sequence_train_shadow = LibriSpeechDataset(path, df = dfs[2], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "\n",
    "\n",
    "valid_sequence_test_shadow = LibriSpeechDataset(path, df = dfs[3], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "\n",
    "valid_sequence_out = LibriSpeechDataset(path, df = dfs[4], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)\n",
    "\n",
    "valid_sequence_out_shadow = LibriSpeechDataset(path, df = dfs[5], seconds = n_seconds, downsampling=1, \n",
    "                                    transform = transform, stochastic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Loaders for data for target model & shadow model \n",
    "target_train_loader = DataLoader(valid_sequence_train,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8,\n",
    "                    drop_last = True\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "target_out_loader = DataLoader(valid_sequence_out,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8,\n",
    "                    drop_last = True\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "target_test_loader = DataLoader(valid_sequence_test,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_train_loader = DataLoader(valid_sequence_train_shadow,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8,\n",
    "                    drop_last = True\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_out_loader = DataLoader(valid_sequence_out_shadow,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8,\n",
    "                    drop_last = True\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n",
    "\n",
    "shadow_test_loader = DataLoader(valid_sequence_test_shadow,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8\n",
    "                     # pin_memory=True # CUDA only\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Targets\n",
    "The model being attacked; if network, architecture can differ from that of shadow network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize NN\n",
    "\n",
    "#in_size defined above\n",
    "n_hidden = 512\n",
    "n_classes = valid_sequence_test.num_speakers\n",
    "print(n_classes)\n",
    "\n",
    "target_net = target_net_type(in_size,n_hidden,n_classes).to(device)\n",
    "target_net.apply(models.weights_init)\n",
    "\n",
    "target_loss = nn.CrossEntropyLoss()\n",
    "target_optim = optim.Adam(target_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Train NN\n",
    "if not pretrained:\n",
    "    train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs, verbose = False) #classes = range(valid_sequence_test.num_speakers),\n",
    "else:\n",
    "    fnm = 'model_weights/CNN_voice_classifier'+str(data_set)+'_target_'+transform_type+str(n_epochs-1)+'.pth'\n",
    "#     fnm = 'model_weights/CNN_voice_classifier100_target_'+str(epoch)+'.pth'\n",
    "    chpt = torch.load(fnm)\n",
    "    target_net.load_state_dict(chpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "epoch = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pretrained:\n",
    "\n",
    "    save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'arch': 'audio_CNN_classifier',\n",
    "                'state_dict': target_net.state_dict(),\n",
    "                'optimizer' : target_optim.state_dict(),\n",
    "            }, False, filename = 'model_weights/CNN_voice_classifier'+str(data_set)+'_target_'+transform_type+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold off on adapting SVM for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize SVM\n",
    "\n",
    "# # #The stored baseline SVM was fit using all of CIFAR10 training data. To attack for membership inference, use \n",
    "# # #images not in CIFAR10 training data, or fit new classifiers/run source code with subset of CIFAR10.\n",
    "\n",
    "# # '''\n",
    "# # dir='../../../Classification_baselines/CIFAR10'\n",
    "# # target_gen=load_svm(dir, gen=True)\n",
    "# # target_maxacc=load_svm(dir, gen=False)\n",
    "# # '''\n",
    "\n",
    "# # #Training example targets on loaded CIFAR10 target subset:\n",
    "\n",
    "# gen_svm=make_pipeline(PCA(n_components=180), MinMaxScaler(feature_range=(-1,1)), svm.SVC(C=10, gamma=.1, probability=True))\n",
    "# maxacc_svm=make_pipeline(PCA(n_components=180), MinMaxScaler(feature_range=(-1,1)), svm.SVC(C=1, gamma=.01, probability=True))\n",
    "\n",
    "# # sv_target_fit_loader = torch.utils.data.DataLoader(sv_cifar10_trainset, batch_size=target_train_sampler.__len__(), \n",
    "# #                                                    sampler=target_train_sampler, num_workers=1)\n",
    "\n",
    "\n",
    "# tin, tout=load(target_train_loader)\n",
    "\n",
    "# #Train SVM\n",
    "# gen_svm.fit(tin, tout)\n",
    "# maxacc_svm.fit(tin, tout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #evaluate SVM targets\n",
    "\n",
    "# classes = range(n_classes)\n",
    "# inp, outp=load(target_test_loader)\n",
    "\n",
    "# print('SVM A (C=', gen_svm.get_params(deep=True)['svc__C'], ', gamma= ',\n",
    "#       gen_svm.get_params(deep=True)['svc__gamma'], '): ')\n",
    "# class_acc(gen_svm.predict_proba(inp), outp, classes)\n",
    "\n",
    "# print('SVM B (C=', maxacc_svm.get_params(deep=True)['svc__C'], ', gamma= ',\n",
    "#       maxacc_svm.get_params(deep=True)['svc__gamma'], '): ')\n",
    "# class_acc(maxacc_svm.predict_proba(inp), outp, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Train Shadow Model\n",
    "Shadow model mimics the target network, emulating the target model's differences in prediction probabilities for samples in and out of its dataset. For this attack, only one shadow model is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize models\n",
    "\n",
    "n_classes = valid_sequence_test_shadow.num_speakers\n",
    "print(n_classes)\n",
    "\n",
    "#NN\n",
    "shadow_net = shadow_net_type(in_size,n_hidden,n_classes).to(device)\n",
    "shadow_net.apply(models.weights_init)\n",
    "\n",
    "shadow_loss = nn.CrossEntropyLoss()\n",
    "shadow_optim = optim.Adam(shadow_net.parameters(), lr=lr)\n",
    "\n",
    "#SVM\n",
    "# shadowinputs, shadowtargets=load(sv_shadow_train_loader)\n",
    "# shadow_svm=make_pipeline(PCA(n_components=180), MinMaxScaler(feature_range=(-1,1)), \n",
    "#                          svm.SVC(C=1, gamma=.1, probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(shadowinputs))\n",
    "# shadowtargets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train NN\n",
    "\n",
    "# below commented code is for comparison during debugging\n",
    "# target_net = target_net_type(in_size,n_hidden,n_classes).to(device)\n",
    "# target_net.apply(models.weights_init)\n",
    "\n",
    "# target_loss = nn.CrossEntropyLoss()\n",
    "# target_optim = optim.Adam(target_net.parameters(), lr=lr)\n",
    "\n",
    "# train(target_net, target_train_loader, target_test_loader, target_optim, target_loss, n_epochs, verbose = False) #classes = range(valid_sequence_test.num_speakers),\n",
    "\n",
    "if not pretrained:\n",
    "    train(shadow_net, shadow_train_loader, shadow_test_loader, shadow_optim, shadow_loss, n_epochs, verbose = False)\n",
    "else:\n",
    "    fnm = 'model_weights/CNN_voice_classifier'+str(data_set)+'_shadow_'+transform_type+str(n_epochs-1)+'.pth'\n",
    "#     fnm = 'model_weights/CNN_voice_classifier100_target_'+str(epoch)+'.pth'\n",
    "    chpt = torch.load(fnm)\n",
    "    shadow_net.load_state_dict(chpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 24\n",
    "save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': 'CNN_voice_classifier',\n",
    "            'state_dict': shadow_net.state_dict(),\n",
    "            'optimizer' : shadow_optim.state_dict(),\n",
    "        }, False, filename = 'model_weights/CNN_voice_classifier'+str(data_set)+'_shadow_'+transform_type+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train SVM\n",
    "# shadow_svm.fit(shadowinputs, shadowtargets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Attack Model\n",
    "A binary classifier to determine membership. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates two attack nets for comparison.\n",
    "\n",
    "# attack_net_svm = models.mlleaks_mlp(n_in=k).to(device)\n",
    "# attack_net_svm.apply(models.weights_init)\n",
    "\n",
    "attack_net_nn = models.mlleaks_mlp(n_in=k).to(device)\n",
    "attack_net_nn.apply(models.weights_init)\n",
    "\n",
    "attack_loss = nn.BCEWithLogitsLoss() #this one works\n",
    "# attack_loss = nn.BCELoss() # this one doesn't work \n",
    "# attack_optim_svm= optim.Adam(attack_net_svm.parameters(), lr=lr)\n",
    "attack_optim_nn= optim.Adam(attack_net_nn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains SVM attack model\n",
    "# train_attacker(attack_net_svm, shadow_svm, shadow_train_loader, shadow_out_loader, attack_optim_svm, attack_loss, n_epochs=2, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains NN attack model\n",
    "attack_loss = nn.BCEWithLogitsLoss()\n",
    "n_epochs_attack = 50\n",
    "\n",
    "if not pretrained:\n",
    "    train_attacker(attack_net_nn, shadow_net, shadow_train_loader, shadow_out_loader, attack_optim_nn, \n",
    "                   attack_loss, n_epochs=n_epochs_attack, k=k)\n",
    "    \n",
    "else:\n",
    "    fnm = 'model_weights/CNN_voice_classifier'+str(data_set)+'_attack_'+transform_type+str(24)+'.pth'\n",
    "#     fnm = 'model_weights/CNN_voice_classifier100_target_'+str(epoch)+'.pth'\n",
    "    chpt = torch.load(fnm)\n",
    "    attack_net_nn.load_state_dict(chpt['state_dict'])\n",
    "\n",
    "#original:\n",
    "# train_attacker(attack_net_nn, shadow_net, shadow_train_loader, shadow_out_loader, attack_optim_nn, attack_loss, n_epochs=50, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "\n",
    "save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': 'CNN_voice_classifier',\n",
    "            'state_dict': attack_net_nn.state_dict(),\n",
    "            'optimizer' : attack_optim_nn.state_dict(),\n",
    "        }, False, filename = 'model_weights/CNN_voice_classifier'+str(data_set)+'_attack_'+transform_type+str(n_epochs_attack)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Attack Nets\n",
    "How well the trained attack models classify a sample as in or out of a target model's training dataset, and how performance is affected by target hyperparameters and which models attack which targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#attack net trained on svm shadow model on svm target, C=10, gamma=.1\n",
    "# eval_attack_net(attack_net_svm, gen_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack net trained on svm shadow model on svm target, C=1, gamma=.01\n",
    "# eval_attack_net(attack_net_svm, maxacc_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#attack net trained on nn shadow model on nn target\n",
    "eval_attack_net(attack_net_nn, target_net, target_train_loader, target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack net trained on nn shadow model on svm target, C=10, gamma=.1\n",
    "# eval_attack_net(attack_net_nn, gen_svm, sv_target_train_loader, sv_target_out_loader, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table with \n",
    "\n",
    "#baselines\n",
    "columns = ['Transform','Training epochs', '# speakers','Train accuracy', 'Test accuracy', 'Attack type', 'Precision','Recall']\n",
    "\n",
    "# do this for 10 & 100 speakers\n",
    "# .2 S & 3 S\n",
    "#sufficient training and over-training\n",
    "\n",
    "#manual data: \n",
    "df = pd.DataFrame(columns = columns)\n",
    "#Attack 1:\n",
    "df.loc[len(df)] = ['MFCC',25,69.0,.9994,.9632,1,0.89,0.90] \n",
    "df.loc[len(df)] = ['MFCC',25,255.0,.9961,.9443,1,0.88,0.91] \n",
    "df.loc[len(df)] = ['STFT',25,69.0,0.9989,0.9451,1,0.89,0.92] \n",
    "df.loc[len(df)] = ['STFT',25,255.0,0.9958,0.9181,1,0.85,0.86] \n",
    "\n",
    "#Attack 3 w/max data: \n",
    "df.loc[len(df)] = ['STFT',25,139.0,.9985,.9073,3,.81,.90] \n",
    "df.loc[len(df)] = ['STFT',50,511.0,.9942,.9057,3,.84,.87] \n",
    "df.loc[len(df)] = ['MFCC',25,139.0,.9969,.9136,3,.82,.92] \n",
    "df.loc[len(df)] = ['MFCC',25,511.0,.9960,.9321,3,0.83,0.93]\n",
    "\n",
    "\n",
    "#Attack 3 on Attack1 models:\n",
    "df.loc[len(df)] = ['MFCC',25,69.0,.9994,.9632,3,0.84,0.95] \n",
    "df.loc[len(df)] = ['MFCC',25,255.0,.9961,.9443,3,0.84,0.94] \n",
    "df.loc[len(df)] = ['STFT',25,69.0,0.9989,0.9451,3,0.81,0.97] \n",
    "df.loc[len(df)] = ['STFT',25,255.0,0.9958,0.9181,3,0.81,0.90] \n",
    "\n",
    "df['Training epochs'] =df['Training epochs'].astype(float)\n",
    "df['Attack type'] =df['Attack type'].astype(float)\n",
    "\n",
    "#style table\n",
    "import seaborn as sns\n",
    "\n",
    "cg = sns.light_palette(\"green\", as_cmap=True)\n",
    "cm = sns.light_palette(\"magenta\", as_cmap=True)\n",
    "bl = sns.light_palette(\"blue\", as_cmap=True)\n",
    "orr = sns.light_palette(\"orange\", as_cmap=True)\n",
    "gr = sns.light_palette(\"gray\", as_cmap=True)\n",
    "\n",
    "# df.style.bar(subset=['Train accuracy', 'Test accuracy'], align='mid', color=['#d65f5f', '#5fba7d'])\n",
    "s = df.style.\\\n",
    "    background_gradient(cmap=cg,subset=['Train accuracy', 'Test accuracy']).\\\n",
    "    background_gradient(cmap=bl,subset=['Precision', 'Recall']).\\\n",
    "    background_gradient(cmap=orr,subset=['Training epochs']).\\\n",
    "    background_gradient(cmap=gr,subset=['Attack type']).\\\n",
    "    background_gradient(cmap=cm,subset=['# speakers']).\\\n",
    "    format({\"Train accuracy\": \"{:.2%}\",\"Test accuracy\": \"{:.2%}\"}).\\\n",
    "    hide_index().\\\n",
    "    set_properties(**{'font-size': \"16pt\",'column-size':\"24pt\",'width': '100px'})\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
