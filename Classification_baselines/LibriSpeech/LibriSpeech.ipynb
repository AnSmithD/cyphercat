{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libri speech Classification baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.5 (default, Jun 21 2018, 23:07:39) \n",
      "[GCC 5.4.0 20160609]\n",
      "Pytorch: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys \n",
    "import os\n",
    "from os import path, makedirs\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "# import torchvision \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "sys.path.insert(0, '../../Utils')\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "import codecs\n",
    "import fnmatch\n",
    "import pandas\n",
    "import requests\n",
    "import progressbar #\n",
    "import subprocess\n",
    "import tarfile\n",
    "import unicodedata\n",
    "\n",
    "#from sox import Transformer #\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "# import models\n",
    "# from train import *\n",
    "# from metrics import * \n",
    "\n",
    "print(\"Python: %s\" % sys.version)\n",
    "print(\"Pytorch: %s\" % torch.__version__)\n",
    "\n",
    "# determine device to run network on (runs on gpu if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LibriSpeech data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Librivox data set (55GB) into -f if not already present...\n",
      "No archive \"-f/train-clean-100.tar.gz\" - downloading...\n"
     ]
    }
   ],
   "source": [
    "#code adapted from https://github.com/mozilla/DeepSpeech/blob/master/bin/import_librivox.py\n",
    "#code in progress\n",
    "\n",
    "def maybe_download(archive_name, target_dir, archive_url):\n",
    "    # If archive file does not exist, download it...\n",
    "    archive_path = path.join(target_dir, archive_name)\n",
    "\n",
    "    if not path.exists(target_dir):\n",
    "        print('No path \"%s\" - creating ...' % target_dir)\n",
    "        makedirs(target_dir)\n",
    "\n",
    "    if not path.exists(archive_path):\n",
    "        print('No archive \"%s\" - downloading...' % archive_path)\n",
    "        req = requests.get(archive_url, stream=True)\n",
    "        total_size = int(req.headers.get('content-length', 0))\n",
    "        done = 0\n",
    "        with open(archive_path, 'wb') as f:\n",
    "            for data in req.iter_content(1024*1024):\n",
    "                done += len(data)\n",
    "                f.write(data)\n",
    "    else:\n",
    "        print('Found archive \"%s\" - not downloading.' % archive_path)\n",
    "    return archive_path\n",
    "\n",
    "def _download_and_preprocess_data(data_dir):\n",
    "    # Conditionally download data to data_dir\n",
    "    print(\"Downloading Librivox data set (55GB) into {} if not already present...\".format(data_dir))\n",
    "   \n",
    "    TRAIN_CLEAN_100_URL = \"http://www.openslr.org/resources/12/train-clean-100.tar.gz\"\n",
    "#     TRAIN_CLEAN_360_URL = \"http://www.openslr.org/resources/12/train-clean-360.tar.gz\"\n",
    "#     TRAIN_OTHER_500_URL = \"http://www.openslr.org/resources/12/train-other-500.tar.gz\"\n",
    "\n",
    "    DEV_CLEAN_URL = \"http://www.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "    DEV_OTHER_URL = \"http://www.openslr.org/resources/12/dev-other.tar.gz\"\n",
    "\n",
    "    TEST_CLEAN_URL = \"http://www.openslr.org/resources/12/test-clean.tar.gz\"\n",
    "    TEST_OTHER_URL = \"http://www.openslr.org/resources/12/test-other.tar.gz\"\n",
    "\n",
    "    def filename_of(x): return os.path.split(x)[1]\n",
    "    train_clean_100 = maybe_download(filename_of(TRAIN_CLEAN_100_URL), data_dir, TRAIN_CLEAN_100_URL)\n",
    "#     train_clean_360 = maybe_download(filename_of(TRAIN_CLEAN_360_URL), data_dir, TRAIN_CLEAN_360_URL)\n",
    "#     train_other_500 = maybe_download(filename_of(TRAIN_OTHER_500_URL), data_dir, TRAIN_OTHER_500_URL)\n",
    "\n",
    "    dev_clean = maybe_download(filename_of(DEV_CLEAN_URL), data_dir, DEV_CLEAN_URL)\n",
    "    dev_other = maybe_download(filename_of(DEV_OTHER_URL), data_dir, DEV_OTHER_URL)\n",
    "\n",
    "\n",
    "    test_clean = maybe_download(filename_of(TEST_CLEAN_URL), data_dir, TEST_CLEAN_URL)\n",
    "    test_other = maybe_download(filename_of(TEST_OTHER_URL), data_dir, TEST_OTHER_URL)\n",
    "\n",
    "    # Conditionally extract LibriSpeech data\n",
    "    # We extract each archive into data_dir, but test for existence in\n",
    "    # data_dir/LibriSpeech because the archives share that root.\n",
    "    print(\"Extracting librivox data if not already extracted...\")\n",
    "\n",
    "    LIBRIVOX_DIR = \"LibriSpeech\"\n",
    "    work_dir = os.path.join(data_dir, LIBRIVOX_DIR)\n",
    "\n",
    "    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, \"train-clean-100\"), train_clean_100)\n",
    "#     _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, \"train-clean-360\"), train_clean_360)\n",
    "#     _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, \"train-other-500\"), train_other_500)\n",
    "\n",
    "    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, \"dev-clean\"), dev_clean)\n",
    "    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, \"dev-other\"), dev_other)\n",
    "\n",
    "    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, \"test-clean\"), test_clean)\n",
    "    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, \"test-other\"), test_other)\n",
    "\n",
    "    # Convert FLAC data to wav, from:\n",
    "    #  data_dir/LibriSpeech/split/1/2/1-2-3.flac\n",
    "    # to:\n",
    "    #  data_dir/LibriSpeech/split-wav/1-2-3.wav\n",
    "    #\n",
    "    # And split LibriSpeech transcriptions, from:\n",
    "    #  data_dir/LibriSpeech/split/1/2/1-2.trans.txt\n",
    "    # to:\n",
    "    #  data_dir/LibriSpeech/split-wav/1-2-0.txt\n",
    "    #  data_dir/LibriSpeech/split-wav/1-2-1.txt\n",
    "    #  data_dir/LibriSpeech/split-wav/1-2-2.txt\n",
    "    #  ...\n",
    "    print(\"Converting FLAC to WAV and splitting transcriptions...\")\n",
    "\n",
    "    train_100 = _convert_audio_and_split_sentences(work_dir, \"train-clean-100\", \"train-clean-100-wav\")\n",
    "#     train_360 = _convert_audio_and_split_sentences(work_dir, \"train-clean-360\", \"train-clean-360-wav\")\n",
    "#     train_500 = _convert_audio_and_split_sentences(work_dir, \"train-other-500\", \"train-other-500-wav\")\n",
    "\n",
    "    dev_clean = _convert_audio_and_split_sentences(work_dir, \"dev-clean\", \"dev-clean-wav\")\n",
    "    dev_other = _convert_audio_and_split_sentences(work_dir, \"dev-other\", \"dev-other-wav\")\n",
    "\n",
    "    test_clean = _convert_audio_and_split_sentences(work_dir, \"test-clean\", \"test-clean-wav\")\n",
    "    test_other = _convert_audio_and_split_sentences(work_dir, \"test-other\", \"test-other-wav\")\n",
    "\n",
    "    # Write sets to disk as CSV files\n",
    "#     train_100.to_csv(os.path.join(data_dir, \"librivox-train-clean-100.csv\"), index=False)\n",
    "#     train_360.to_csv(os.path.join(data_dir, \"librivox-train-clean-360.csv\"), index=False)\n",
    "    train_500.to_csv(os.path.join(data_dir, \"librivox-train-other-500.csv\"), index=False)\n",
    "\n",
    "    dev_clean.to_csv(os.path.join(data_dir, \"librivox-dev-clean.csv\"), index=False)\n",
    "    dev_other.to_csv(os.path.join(data_dir, \"librivox-dev-other.csv\"), index=False)\n",
    "\n",
    "    test_clean.to_csv(os.path.join(data_dir, \"librivox-test-clean.csv\"), index=False)\n",
    "    test_other.to_csv(os.path.join(data_dir, \"librivox-test-other.csv\"), index=False)\n",
    "\n",
    "def _maybe_extract(data_dir, extracted_data, archive):\n",
    "    # If data_dir/extracted_data does not exist, extract archive in data_dir\n",
    "    if not gfile.Exists(os.path.join(data_dir, extracted_data)):\n",
    "        tar = tarfile.open(archive)\n",
    "        tar.extractall(data_dir)\n",
    "        tar.close()\n",
    "\n",
    "def _convert_audio_and_split_sentences(extracted_dir, data_set, dest_dir):\n",
    "    source_dir = os.path.join(extracted_dir, data_set)\n",
    "    target_dir = os.path.join(extracted_dir, dest_dir)\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    # Loop over transcription files and split each one\n",
    "    #\n",
    "    # The format for each file 1-2.trans.txt is:\n",
    "    #  1-2-0 transcription of 1-2-0.flac\n",
    "    #  1-2-1 transcription of 1-2-1.flac\n",
    "    #  ...\n",
    "    #\n",
    "    # Each file is then split into several files:\n",
    "    #  1-2-0.txt (contains transcription of 1-2-0.flac)\n",
    "    #  1-2-1.txt (contains transcription of 1-2-1.flac)\n",
    "    #  ...\n",
    "    #\n",
    "    # We also convert the corresponding FLACs to WAV in the same pass\n",
    "    files = []\n",
    "    for root, dirnames, filenames in os.walk(source_dir):\n",
    "        for filename in fnmatch.filter(filenames, '*.trans.txt'):\n",
    "            trans_filename = os.path.join(root, filename)\n",
    "            with codecs.open(trans_filename, \"r\", \"utf-8\") as fin:\n",
    "                for line in fin:\n",
    "                    # Parse each segment line\n",
    "                    first_space = line.find(\" \")\n",
    "                    seqid, transcript = line[:first_space], line[first_space+1:]\n",
    "\n",
    "                    # We need to do the encode-decode dance here because encode\n",
    "                    # returns a bytes() object on Python 3, and text_to_char_array\n",
    "                    # expects a string.\n",
    "                    transcript = unicodedata.normalize(\"NFKD\", transcript)  \\\n",
    "                                            .encode(\"ascii\", \"ignore\")      \\\n",
    "                                            .decode(\"ascii\", \"ignore\")\n",
    "\n",
    "                    transcript = transcript.lower().strip()\n",
    "\n",
    "                    # Convert corresponding FLAC to a WAV\n",
    "                    flac_file = os.path.join(root, seqid + \".flac\")\n",
    "                    wav_file = os.path.join(target_dir, seqid + \".wav\")\n",
    "                    if not os.path.exists(wav_file):\n",
    "                        Transformer().build(flac_file, wav_file)\n",
    "                    wav_filesize = os.path.getsize(wav_file)\n",
    "\n",
    "                    files.append((os.path.abspath(wav_file), wav_filesize, transcript))\n",
    "\n",
    "    return pandas.DataFrame(data=files, columns=[\"wav_filename\", \"wav_filesize\", \"transcript\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _download_and_preprocess_data(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
